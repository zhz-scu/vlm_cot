#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›çš„Chain-of-Spotæ¨¡å‹

é›†æˆç›®æ ‡æ£€æµ‹ç®—æ³•è¿›è¡ŒROIè¯†åˆ«ï¼Œç„¶åé€šè¿‡æ¨¡å‹æ ‡æ³¨å†…å®¹
"""

import torch
import numpy as np
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass
from PIL import Image, ImageDraw
import re
import json
import time
import cv2
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
    HAS_NATIVE_QWEN25_VL = True
except ImportError:
    from transformers import AutoModelForCausalLM, AutoProcessor
    Qwen2_5_VLForConditionalGeneration = None
    HAS_NATIVE_QWEN25_VL = False


@dataclass
class DetectedRegion:
    """æ£€æµ‹åˆ°çš„åŒºåŸŸ"""
    bbox: List[float]  # [x0, y0, x1, y1] å½’ä¸€åŒ–åæ ‡
    confidence: float
    label: str
    content: str
    region_id: int


@dataclass
class ImprovedCoSResponse:
    """æ”¹è¿›çš„CoSå“åº”"""
    detected_regions: List[DetectedRegion]
    final_answer: str
    reasoning_trace: List[str]
    confidence: float
    detection_time: float
    annotation_time: float


class ObjectDetector:
    """ç›®æ ‡æ£€æµ‹å™¨"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.model = None
        self.initialized = False
    
    def initialize_detector(self, model_type: str = "edge"):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        try:
            if model_type == "yolo":
                try:
                    from ultralytics import YOLO
                    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æƒé‡
                    local_weight = os.path.join(os.path.dirname(__file__), '..', '..', 'scripts', 'yolo', 'yolov8n.pt')
                    local_weight = os.path.abspath(local_weight)
                    if os.path.exists(local_weight):
                        self.model = YOLO(local_weight)
                        print(f"âœ… YOLOv8æ£€æµ‹å™¨åŠ è½½æœ¬åœ°æƒé‡: {local_weight}")
                    else:
                        self.model = YOLO('yolov8n.pt')
                        print("âœ… YOLOv8æ£€æµ‹å™¨åŠ è½½é»˜è®¤æƒé‡ yolov8n.pt")
                    self.initialized = True
                    print("âœ… YOLOv8æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
                except ImportError:
                    print("âš ï¸ æœªå®‰è£…ultralyticsï¼Œä½¿ç”¨è¾¹ç¼˜æ£€æµ‹")
                    self._init_edge_detector()
            else:
                self._init_edge_detector()
        except Exception as e:
            print(f"âŒ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.initialized = False
    
    def _init_edge_detector(self):
        """åˆå§‹åŒ–è¾¹ç¼˜æ£€æµ‹å™¨"""
        self.model = "edge_detection"
        self.initialized = True
        print("âœ… è¾¹ç¼˜æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def detect_regions(self, image: Image.Image, min_confidence: float = 0.3) -> List[DetectedRegion]:
        """æ£€æµ‹å›¾åƒä¸­çš„åŒºåŸŸ"""
        if not self.initialized:
            self.initialize_detector()
        
        if isinstance(self.model, str) and self.model == "edge_detection":
            return self._edge_detection(image)
        elif hasattr(self.model, 'predict'):
            return self._yolo_detection(image, min_confidence)
        else:
            return self._edge_detection(image)
    
    def _edge_detection(self, image: Image.Image) -> List[DetectedRegion]:
        """è¾¹ç¼˜æ£€æµ‹æ–¹æ³•"""
        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        regions = []
        img_height, img_width = gray.shape
        
        for i, contour in enumerate(contours):
            x, y, w, h = cv2.boundingRect(contour)
            if w < 20 or h < 20:
                continue
            
            x0 = x / img_width
            y0 = y / img_height
            x1 = (x + w) / img_width
            y1 = (y + h) / img_height
            
            area = w * h
            confidence = min(0.8, area / (img_width * img_height) * 10)
            
            regions.append(DetectedRegion(
                bbox=[x0, y0, x1, y1],
                confidence=confidence,
                label=f"region_{i}",
                content="",
                region_id=i
            ))
        
        return regions[:5]
    
    def _yolo_detection(self, image: Image.Image, min_confidence: float) -> List[DetectedRegion]:
        """YOLOæ£€æµ‹æ–¹æ³•"""
        try:
            results = self.model(image)
            regions = []
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for i, box in enumerate(boxes):
                        confidence = float(box.conf[0])
                        if confidence < min_confidence:
                            continue
                        
                        x0, y0, x1, y1 = box.xyxy[0].cpu().numpy()
                        img_width, img_height = image.size
                        
                        x0_norm = x0 / img_width
                        y0_norm = y0 / img_height
                        x1_norm = x1 / img_width
                        y1_norm = y1 / img_height
                        
                        class_id = int(box.cls[0])
                        label = f"object_{class_id}"
                        
                        regions.append(DetectedRegion(
                            bbox=[x0_norm, y0_norm, x1_norm, y1_norm],
                            confidence=confidence,
                            label=label,
                            content="",
                            region_id=i
                        ))
            
            return regions
        except Exception as e:
            print(f"YOLOæ£€æµ‹å¤±è´¥: {e}")
            return self._edge_detection(image)


class RegionAnnotator:
    """åŒºåŸŸæ ‡æ³¨å™¨"""
    
    def __init__(self, model, processor, device: str):
        self.model = model
        self.processor = processor
        self.device = device
    
    def annotate_regions(self, image: Image.Image, regions: List[DetectedRegion]) -> List[DetectedRegion]:
        """æ ‡æ³¨æ£€æµ‹åˆ°çš„åŒºåŸŸ"""
        annotated_regions = []
        
        for region in regions:
            x0, y0, x1, y1 = region.bbox
            img_width, img_height = image.size
            
            x0_pixel = max(0, min(int(x0 * img_width), img_width))
            y0_pixel = max(0, min(int(y0 * img_height), img_height))
            x1_pixel = max(x0_pixel, min(int(x1 * img_width), img_width))
            y1_pixel = max(y0_pixel, min(int(y1 * img_height), img_height))
            
            if x1_pixel <= x0_pixel or y1_pixel <= y0_pixel:
                continue
            
            try:
                region_image = image.crop((x0_pixel, y0_pixel, x1_pixel, y1_pixel))
                content = self._annotate_single_region(region_image, region.label)
                
                annotated_regions.append(DetectedRegion(
                    bbox=region.bbox,
                    confidence=region.confidence,
                    label=region.label,
                    content=content,
                    region_id=region.region_id
                ))
            except Exception as e:
                print(f"åŒºåŸŸæ ‡æ³¨å¤±è´¥: {e}")
                continue
        
        return annotated_regions
    
    def _annotate_single_region(self, region_image: Image.Image, label: str) -> str:
        """æ ‡æ³¨å•ä¸ªåŒºåŸŸ"""
        try:
            prompt = f"è¯·æè¿°è¿™ä¸ªå›¾åƒåŒºåŸŸä¸­çš„å†…å®¹ï¼Œç®€æ´æ˜äº†ï¼š"
            
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": region_image},
                    {"type": "text", "text": prompt}
                ]
            }]
            
            try:
                chat_text = self.processor.apply_chat_template(
                    messages=messages, tokenize=False, add_generation_prompt=True
                )
            except Exception:
                chat_text = self.processor.apply_chat_template(
                    conversation=messages, tokenize=False, add_generation_prompt=True
                )
            
            inputs = self.processor(
                text=[chat_text],
                images=[region_image],
                padding=True,
                return_tensors="pt",
            )
            
            if self.device in ("cuda", "mps", "npu", "xpu"):
                inputs = {k: v.to(self.device) if hasattr(v, "to") else v for k, v in inputs.items()}
            
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    temperature=0.1,
                )
            
            try:
                trimmed = [out[len(inp):] for inp, out in zip(inputs["input_ids"], generated_ids)]
                content = self.processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            except Exception:
                content = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            
            return content.strip()
            
        except Exception as e:
            print(f"åŒºåŸŸæ ‡æ³¨å¤±è´¥: {e}")
            return f"åŒºåŸŸå†…å®¹: {label}"


class ImprovedChainOfSpotModel:
    """æ”¹è¿›çš„Chain-of-Spotæ¨¡å‹"""
    
    def __init__(self, model_id: str, device: str = "auto", detector_type: str = "yolo"):
        self.device = self._auto_select_device(device)
        self.detector_type = detector_type
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.object_detector = ObjectDetector(self.device)
        self.object_detector.initialize_detector(detector_type)
        
        # åŠ è½½æ¨¡å‹
        self.model, self.processor = self._load_model(model_id)
        
        # åˆå§‹åŒ–æ ‡æ³¨å™¨
        self.region_annotator = RegionAnnotator(self.model, self.processor, self.device)
    
    def _auto_select_device(self, device: str) -> str:
        """è‡ªåŠ¨é€‰æ‹©è®¾å¤‡"""
        if device != "auto":
            return device
        
        if hasattr(torch, 'npu') and torch.npu.is_available():
            return "npu"
        elif torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch, 'xpu') and torch.xpu.is_available():
            return "xpu"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    def _load_model(self, model_id: str):
        """åŠ è½½æ¨¡å‹"""
        torch_dtype = torch.float16 if self.device in ("npu", "mps", "xpu") else torch.float32
        
        if HAS_NATIVE_QWEN25_VL and Qwen2_5_VLForConditionalGeneration is not None:
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch_dtype,
                device_map="auto" if self.device in ("cuda", "mps", "npu", "xpu") else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch_dtype,
                device_map="auto" if self.device in ("cuda", "mps", "npu", "xpu") else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        
        return model, processor
    
    def improved_reasoning(self, image: Image.Image, question: str) -> ImprovedCoSResponse:
        """æ”¹è¿›çš„æ¨ç†æµç¨‹"""
        reasoning_trace = []
        start_time = time.time()
        
        # æ­¥éª¤1: ç›®æ ‡æ£€æµ‹
        reasoning_trace.append("å¼€å§‹ç›®æ ‡æ£€æµ‹...")
        detection_start = time.time()
        detected_regions = self.object_detector.detect_regions(image, min_confidence=0.3)
        detection_time = time.time() - detection_start
        reasoning_trace.append(f"æ£€æµ‹åˆ° {len(detected_regions)} ä¸ªåŒºåŸŸï¼Œè€—æ—¶ {detection_time:.2f}ç§’")

        # å¯è§†åŒ–å¹¶ä¿å­˜æ£€æµ‹ç»“æœ
        try:
            out_dir = "/test_res"
            os.makedirs(out_dir, exist_ok=True)
            viz_img = _create_visualization(image, detected_regions)
            out_path = os.path.join(out_dir, f"detection_{self.detector_type}.png")
            viz_img.save(out_path)
            print(f"âœ… æ£€æµ‹å¯è§†åŒ–å·²ä¿å­˜: {out_path}")
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹å¯è§†åŒ–ä¿å­˜å¤±è´¥: {e}")

        # è¾“å‡ºæ£€æµ‹ç»“æœè¯¦æƒ…
        if detected_regions:
            print("ğŸ“‹ æ£€æµ‹ç»“æœæ˜ç»†:")
            for r in detected_regions:
                print(f"  - id={r.region_id}, label={r.label}, conf={r.confidence:.3f}, bbox={r.bbox}")
        else:
            print("ğŸ“‹ æ£€æµ‹ç»“æœæ˜ç»†: æ— æ£€æµ‹æ¡†")
        
        # æ­¥éª¤2: åŒºåŸŸæ ‡æ³¨
        reasoning_trace.append("å¼€å§‹åŒºåŸŸæ ‡æ³¨...")
        annotation_start = time.time()
        annotated_regions = self.region_annotator.annotate_regions(image, detected_regions)
        annotation_time = time.time() - annotation_start
        reasoning_trace.append(f"å®ŒæˆåŒºåŸŸæ ‡æ³¨ï¼Œè€—æ—¶ {annotation_time:.2f}ç§’")
        
        # æ­¥éª¤3: åŸºäºæ ‡æ³¨çš„æ¨ç†
        reasoning_trace.append("å¼€å§‹åŸºäºæ ‡æ³¨çš„æ¨ç†...")
        final_answer = self._reason_with_annotations(image, question, annotated_regions)
        reasoning_trace.append("æ¨ç†å®Œæˆ")
        
        total_time = time.time() - start_time
        reasoning_trace.append(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = np.mean([region.confidence for region in annotated_regions]) if annotated_regions else 0.5
        
        return ImprovedCoSResponse(
            detected_regions=annotated_regions,
            final_answer=final_answer,
            reasoning_trace=reasoning_trace,
            confidence=confidence,
            detection_time=detection_time,
            annotation_time=annotation_time
        )
    
    def _reason_with_annotations(self, image: Image.Image, question: str, regions: List[DetectedRegion]) -> str:
        """åŸºäºæ ‡æ³¨è¿›è¡Œæ¨ç†"""
        region_info = []
        for region in regions:
            region_info.append(f"åŒºåŸŸ{region.region_id}: {region.content} (ç½®ä¿¡åº¦: {region.confidence:.2f})")
        
        region_text = "\n".join(region_info) if region_info else "æœªæ£€æµ‹åˆ°æ˜æ˜¾åŒºåŸŸ"
        
        prompt = f"""åŸºäºä»¥ä¸‹æ£€æµ‹åˆ°çš„åŒºåŸŸä¿¡æ¯ï¼Œè¯·å›ç­”é—®é¢˜ï¼š

æ£€æµ‹åˆ°çš„åŒºåŸŸï¼š
{region_text}

é—®é¢˜ï¼š{question}

è¯·åŸºäºåŒºåŸŸä¿¡æ¯è¿›è¡Œè¯¦ç»†åˆ†æå¹¶ç»™å‡ºç­”æ¡ˆã€‚"""

        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": prompt}
            ]
        }]
        
        try:
            chat_text = self.processor.apply_chat_template(
                messages=messages, tokenize=False, add_generation_prompt=True
            )
        except Exception:
            chat_text = self.processor.apply_chat_template(
                conversation=messages, tokenize=False, add_generation_prompt=True
            )
        
        inputs = self.processor(
            text=[chat_text],
            images=[image],
            padding=True,
            return_tensors="pt",
        )
        
        if self.device in ("cuda", "mps", "npu", "xpu"):
            inputs = {k: v.to(self.device) if hasattr(v, "to") else v for k, v in inputs.items()}
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=512,
                do_sample=True,
                temperature=0.3,
                top_p=0.9,
            )
        
        try:
            trimmed = [out[len(inp):] for inp, out in zip(inputs["input_ids"], generated_ids)]
            answer = self.processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        except Exception:
            answer = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        
        return answer.strip()


def improved_cos_generate(
    model_id: str,
    image_path: str,
    question: str,
    device: str = "auto",
    detector_type: str = "edge",
    save_visualization: bool = False,
    output_dir: str = ".",
) -> Dict[str, Any]:
    """æ”¹è¿›çš„CoSç”Ÿæˆå‡½æ•°"""
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    
    # åˆå§‹åŒ–æ”¹è¿›çš„CoSæ¨¡å‹
    improved_cos = ImprovedChainOfSpotModel(model_id, device, detector_type)
    
    # æ‰§è¡Œæ”¹è¿›æ¨ç†
    response = improved_cos.improved_reasoning(image, question)
    
    # ä¿å­˜å¯è§†åŒ–ç»“æœ
    if save_visualization:
        viz_image = _create_visualization(image, response.detected_regions)
        viz_path = f"{output_dir}/improved_cos_visualization.png"
        viz_image.save(viz_path)
    
    # æ„å»ºè¿”å›ç»“æœ
    result = {
        "method": "Improved Chain-of-Spot",
        "device": device,
        "detector_type": detector_type,
        "question": question,
        "image_path": image_path,
        "detected_regions": [
            {
                "region_id": region.region_id,
                "bbox": region.bbox,
                "confidence": region.confidence,
                "label": region.label,
                "content": region.content
            }
            for region in response.detected_regions
        ],
        "final_answer": response.final_answer,
        "confidence": response.confidence,
        "detection_time": response.detection_time,
        "annotation_time": response.annotation_time,
        "reasoning_trace": response.reasoning_trace,
        "improvements": [
            "ç›®æ ‡æ£€æµ‹ç®—æ³•è¯†åˆ«åŒºåŸŸ",
            "æ¨¡å‹æ ‡æ³¨åŒºåŸŸå†…å®¹",
            "åŸºäºæ ‡æ³¨çš„æ¨ç†",
            "æ›´å‡†ç¡®çš„ROIå®šä½"
        ]
    }
    
    return result


def _create_visualization(image: Image.Image, regions: List[DetectedRegion]) -> Image.Image:
    """åˆ›å»ºå¯è§†åŒ–ç»“æœ"""
    viz_image = image.copy()
    draw = ImageDraw.Draw(viz_image)
    
    colors = ["red", "blue", "green", "yellow", "purple", "orange"]
    
    for i, region in enumerate(regions):
        color = colors[i % len(colors)]
        x0, y0, x1, y1 = region.bbox
        
        img_width, img_height = image.size
        x0_pixel = int(x0 * img_width)
        y0_pixel = int(y0 * img_height)
        x1_pixel = int(x1 * img_width)
        y1_pixel = int(y1 * img_height)
        
        draw.rectangle([x0_pixel, y0_pixel, x1_pixel, y1_pixel], outline=color, width=3)
        
        label = f"R{region.region_id}: {region.content[:20]}..."
        draw.text((x0_pixel, y0_pixel - 20), label, fill=color)
    
    return viz_image


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ”¹è¿›çš„Chain-of-Spotæ¨ç†")
    parser.add_argument("--model-id", type=str, default="Qwen/Qwen2.5-VL-3B-Instruct")
    parser.add_argument("--image", type=str, required=True, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--question", type=str, required=True, help="é—®é¢˜")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡")
    parser.add_argument("--detector", type=str, default="yolo", choices=["yolo", "opencv", "edge"], help="æ£€æµ‹å™¨ç±»å‹")
    parser.add_argument("--save-viz", action="store_true", help="ä¿å­˜å¯è§†åŒ–ç»“æœ")
    
    args = parser.parse_args()
    
    result = improved_cos_generate(
        model_id=args.model_id,
        image_path=args.image,
        question=args.question,
        device=args.device,
        detector_type=args.detector,
        save_visualization=args.save_viz
    )
    
    print("=" * 80)
    print("ğŸ”¬ æ”¹è¿›çš„Chain-of-Spotç»“æœ")
    print("=" * 80)
    print(f"è®¾å¤‡: {result['device']}")
    print(f"æ£€æµ‹å™¨: {result['detector_type']}")
    print(f"é—®é¢˜: {result['question']}")
    print(f"æ£€æµ‹åŒºåŸŸæ•°: {len(result['detected_regions'])}")
    print(f"æ£€æµ‹è€—æ—¶: {result['detection_time']:.2f}ç§’")
    print(f"æ ‡æ³¨è€—æ—¶: {result['annotation_time']:.2f}ç§’")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
    print(f"æœ€ç»ˆç­”æ¡ˆ: {result['final_answer']}")
    
    print("\nğŸ“Š æ£€æµ‹åˆ°çš„åŒºåŸŸ:")
    for region in result['detected_regions']:
        print(f"  åŒºåŸŸ{region['region_id']}: {region['content']} (ç½®ä¿¡åº¦: {region['confidence']:.3f})")
    
    print("\nâœ¨ æ”¹è¿›ç‰¹æ€§:")
    for improvement in result['improvements']:
        print(f"  - {improvement}")
