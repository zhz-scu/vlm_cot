#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chain-of-Spot + VoT æ··åˆæ–¹æ³•: å¯è§†åŒ–äº¤äº’å¼æ¨ç†

åˆ›æ–°ç‚¹ï¼š
1. ç»“åˆCoSçš„ROIå®šä½å’ŒVoTçš„å¯è§†åŒ–æ¨ç†
2. åœ¨ROIå®šä½è¿‡ç¨‹ä¸­åŠ å…¥ç©ºé—´å¯è§†åŒ–
3. å¤šæ­¥éª¤ROIç»†åŒ–ä¸å¯è§†åŒ–éªŒè¯
4. åŠ¨æ€ROIè°ƒæ•´æœºåˆ¶
"""

import torch
import numpy as np
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass
from PIL import Image, ImageDraw, ImageFont
import re
import json

from cos_model import ChainOfSpotModel, BoundingBox, CoSResponse


@dataclass
class VisualROI:
    """å¯è§†åŒ–ROIç»“æ„"""
    bbox: BoundingBox
    confidence: float
    visualization: str  # æ–‡æœ¬å½¢å¼çš„å¯è§†åŒ–
    reasoning: str     # æ¨ç†è¿‡ç¨‹
    step_id: int       # æ­¥éª¤ID


@dataclass
class CoSVoTResponse:
    """CoS+VoTæ··åˆå“åº”"""
    final_roi: BoundingBox
    final_answer: str
    visual_trajectory: List[VisualROI]  # å¯è§†åŒ–è½¨è¿¹
    reasoning_trace: List[str]
    confidence: float
    spatial_visualization: str  # æœ€ç»ˆçš„ç©ºé—´å¯è§†åŒ–


class SpatialVisualizer:
    """ç©ºé—´å¯è§†åŒ–å™¨"""
    
    def __init__(self, grid_size: int = 8):
        self.grid_size = grid_size
        self.grid_chars = {
            'empty': 'Â·',
            'target': 'â˜…',
            'roi': 'â–ˆ',
            'context': 'â–‘',
            'boundary': 'â”‚'
        }
    
    def create_spatial_grid(self, bbox: BoundingBox, target_desc: str = "ç›®æ ‡") -> str:
        """åˆ›å»ºç©ºé—´ç½‘æ ¼å¯è§†åŒ–"""
        grid = []
        for y in range(self.grid_size):
            row = []
            for x in range(self.grid_size):
                # å½’ä¸€åŒ–åæ ‡
                norm_x = x / self.grid_size
                norm_y = y / self.grid_size
                
                if bbox.x0 <= norm_x <= bbox.x1 and bbox.y0 <= norm_y <= bbox.y1:
                    if norm_x == (bbox.x0 + bbox.x1) / 2 and norm_y == (bbox.y0 + bbox.y1) / 2:
                        row.append(self.grid_chars['target'])
                    else:
                        row.append(self.grid_chars['roi'])
                else:
                    row.append(self.grid_chars['empty'])
            grid.append(''.join(row))
        
        # æ·»åŠ è¾¹ç•Œå’Œæ ‡ç­¾
        result = f"ç©ºé—´å¯è§†åŒ– - {target_desc}:\n"
        result += "â”Œ" + "â”€" * self.grid_size + "â”\n"
        for row in grid:
            result += "â”‚" + row + "â”‚\n"
        result += "â””" + "â”€" * self.grid_size + "â”˜\n"
        result += f"åæ ‡: [{bbox.x0:.2f},{bbox.x1:.2f},{bbox.y0:.2f},{bbox.y1:.2f}]\n"
        
        return result
    
    def create_multi_roi_visualization(self, rois: List[VisualROI]) -> str:
        """åˆ›å»ºå¤šROIå¯è§†åŒ–"""
        grid = []
        for y in range(self.grid_size):
            row = []
            for x in range(self.grid_size):
                norm_x = x / self.grid_size
                norm_y = y / self.grid_size
                
                # æ£€æŸ¥æ˜¯å¦åœ¨ä»»ä½•ROIå†…
                in_roi = False
                for i, roi in enumerate(rois):
                    if roi.bbox.x0 <= norm_x <= roi.bbox.x1 and roi.bbox.y0 <= norm_y <= roi.bbox.y1:
                        row.append(str(i + 1))  # ä½¿ç”¨æ•°å­—æ ‡è¯†ä¸åŒROI
                        in_roi = True
                        break
                
                if not in_roi:
                    row.append(self.grid_chars['empty'])
            grid.append(''.join(row))
        
        result = "å¤šROIæ¼”åŒ–å¯è§†åŒ–:\n"
        result += "â”Œ" + "â”€" * self.grid_size + "â”\n"
        for row in grid:
            result += "â”‚" + row + "â”‚\n"
        result += "â””" + "â”€" * self.grid_size + "â”˜\n"
        
        # æ·»åŠ å›¾ä¾‹
        for i, roi in enumerate(rois):
            result += f"ROI{i+1}: ç½®ä¿¡åº¦={roi.confidence:.2f}, æ­¥éª¤={roi.step_id}\n"
        
        return result


class CoSVoTModel(ChainOfSpotModel):
    """Chain-of-Spot + VoT æ··åˆæ¨¡å‹"""
    
    def __init__(self, base_model, processor, device: str = "auto"):
        super().__init__(base_model, processor, device)
        self.spatial_visualizer = SpatialVisualizer()
        
        # æ”¹è¿›çš„æŒ‡ä»¤æ¨¡æ¿
        self.vot_instruction_1 = (
            "<Img> To answer the question: <Q>, "
            "please identify the region of interest and provide a spatial visualization. "
            "Return coordinates as [x0,x1,y0,y1] and create a text-based spatial grid. "
            "Format: COORDS:[x0,x1,y0,y1] VISUAL:[grid visualization]"
        )
        
        self.vot_instruction_2 = (
            "The region of interest is <ROI Img>. "
            "Based on this focused region and the spatial context, "
            "please provide a detailed answer to: <Q>. "
            "Include spatial reasoning in your response."
        )
    
    def _extract_coords_and_visualization(self, response: str) -> Tuple[Optional[BoundingBox], str]:
        """æå–åæ ‡å’Œå¯è§†åŒ–ä¿¡æ¯"""
        bbox = None
        visualization = ""
        
        # æå–åæ ‡
        coords_match = re.search(r'COORDS:\[([\d.]+),([\d.]+),([\d.]+),([\d.]+)\]', response)
        if coords_match:
            coords = [float(x) for x in coords_match.groups()]
            bbox = BoundingBox(x0=coords[0], x1=coords[1], y0=coords[2], y1=coords[3])
        
        # æå–å¯è§†åŒ–
        visual_match = re.search(r'VISUAL:(.*?)(?=COORDS:|$)', response, re.DOTALL)
        if visual_match:
            visualization = visual_match.group(1).strip()
        
        return bbox, visualization
    
    def _multi_step_roi_refinement(self, image: Image.Image, question: str, 
                                 max_steps: int = 3) -> List[VisualROI]:
        """å¤šæ­¥éª¤ROIç»†åŒ–"""
        visual_rois = []
        current_bbox = None
        
        for step in range(max_steps):
            # æ„å»ºå½“å‰æ­¥éª¤çš„æŒ‡ä»¤
            if step == 0:
                instruction = self.vot_instruction_1.replace("<Q>", question)
            else:
                # åŸºäºå‰ä¸€æ­¥ç»“æœè¿›è¡Œç»†åŒ–
                prev_roi = visual_rois[-1]
                instruction = (
                    f"<Img> Previous ROI: {prev_roi.bbox.to_string()} "
                    f"Confidence: {prev_roi.confidence:.2f}\n"
                    f"Question: {question}\n"
                    "Please refine the ROI based on the previous result. "
                    "Provide updated coordinates and visualization."
                )
            
            # è°ƒç”¨æ¨¡å‹
            response = self._call_model([image], instruction)
            
            # æå–ç»“æœ
            bbox, visualization = self._extract_coords_and_visualization(response)
            
            if bbox is None:
                # ä½¿ç”¨å¯å‘å¼æ–¹æ³•
                bbox = self._heuristic_roi_extraction(image, question)
                visualization = self.spatial_visualizer.create_spatial_grid(bbox, f"æ­¥éª¤{step+1}")
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_step_confidence(response, bbox, step)
            
            # åˆ›å»ºVisualROI
            visual_roi = VisualROI(
                bbox=bbox,
                confidence=confidence,
                visualization=visualization,
                reasoning=response,
                step_id=step + 1
            )
            
            visual_rois.append(visual_roi)
            current_bbox = bbox
            
            # å¦‚æœç½®ä¿¡åº¦è¶³å¤Ÿé«˜ï¼Œæå‰åœæ­¢
            if confidence > 0.8:
                break
        
        return visual_rois
    
    def _calculate_step_confidence(self, response: str, bbox: BoundingBox, step: int) -> float:
        """è®¡ç®—æ­¥éª¤ç½®ä¿¡åº¦"""
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦
        
        # åŸºäºå“åº”è´¨é‡
        if "COORDS:" in response and "VISUAL:" in response:
            confidence += 0.2
        
        # åŸºäºROIå¤§å°
        roi_area = (bbox.x1 - bbox.x0) * (bbox.y1 - bbox.y0)
        if 0.1 <= roi_area <= 0.5:
            confidence += 0.1
        elif roi_area < 0.1:
            confidence -= 0.1
        
        # åŸºäºæ­¥éª¤æ•°ï¼ˆè¶Šåé¢çš„æ­¥éª¤è¶Šå¯ä¿¡ï¼‰
        confidence += step * 0.05
        
        return min(confidence, 1.0)
    
    def _dynamic_roi_adjustment(self, visual_rois: List[VisualROI]) -> BoundingBox:
        """åŠ¨æ€ROIè°ƒæ•´"""
        if not visual_rois:
            return BoundingBox(x0=0.25, x1=0.75, y0=0.25, y1=0.75)
        
        # åŸºäºç½®ä¿¡åº¦åŠ æƒå¹³å‡
        total_weight = 0
        weighted_x0 = 0
        weighted_x1 = 0
        weighted_y0 = 0
        weighted_y1 = 0
        
        for roi in visual_rois:
            weight = roi.confidence ** 2  # å¹³æ–¹æƒé‡
            total_weight += weight
            
            weighted_x0 += roi.bbox.x0 * weight
            weighted_x1 += roi.bbox.x1 * weight
            weighted_y0 += roi.bbox.y0 * weight
            weighted_y1 += roi.bbox.y1 * weight
        
        if total_weight > 0:
            final_bbox = BoundingBox(
                x0=weighted_x0 / total_weight,
                x1=weighted_x1 / total_weight,
                y0=weighted_y0 / total_weight,
                y1=weighted_y1 / total_weight
            )
        else:
            final_bbox = visual_rois[-1].bbox
        
        return final_bbox
    
    def visual_interactive_reasoning(self, image: Image.Image, question: str) -> CoSVoTResponse:
        """
        å¯è§†åŒ–äº¤äº’å¼æ¨ç† - ä¸»è¦æ–¹æ³•
        """
        reasoning_trace = []
        
        # æ­¥éª¤1: å¤šæ­¥éª¤ROIç»†åŒ–
        reasoning_trace.append("å¼€å§‹å¤šæ­¥éª¤ROIç»†åŒ–...")
        visual_rois = self._multi_step_roi_refinement(image, question)
        reasoning_trace.append(f"å®Œæˆ{len(visual_rois)}æ­¥ROIç»†åŒ–")
        
        # æ­¥éª¤2: åŠ¨æ€ROIè°ƒæ•´
        final_roi = self._dynamic_roi_adjustment(visual_rois)
        reasoning_trace.append(f"æœ€ç»ˆROI: {final_roi.to_string()}")
        
        # æ­¥éª¤3: è£å‰ªROIå›¾åƒ
        roi_image = self.image_cropper.crop_image(image, final_roi)
        reasoning_trace.append("ROIå›¾åƒè£å‰ªå®Œæˆ")
        
        # æ­¥éª¤4: åŸºäºROIçš„è¯¦ç»†åˆ†æ
        instruction_2 = self.vot_instruction_2.replace("<Q>", question)
        reasoning_trace.append("å¼€å§‹è¯¦ç»†åˆ†æ...")
        
        final_answer = self._call_model([image, roi_image], instruction_2)
        reasoning_trace.append("è¯¦ç»†åˆ†æå®Œæˆ")
        
        # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç©ºé—´å¯è§†åŒ–
        spatial_viz = self.spatial_visualizer.create_multi_roi_visualization(visual_rois)
        
        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦
        final_confidence = np.mean([roi.confidence for roi in visual_rois])
        
        return CoSVoTResponse(
            final_roi=final_roi,
            final_answer=final_answer,
            visual_trajectory=visual_rois,
            reasoning_trace=reasoning_trace,
            confidence=final_confidence,
            spatial_visualization=spatial_viz
        )


def cos_vot_generate(
    model_id: str,
    image_path: str,
    question: str,
    device: str = "auto",
    dtype_str: str = "auto",
    max_new_tokens: int = 512,
    max_roi_steps: int = 3,
    seed: int = None,
    save_visualization: bool = False,
    output_dir: str = ".",
) -> Dict[str, Any]:
    """
    CoS+VoTæ··åˆæ–¹æ³•ç”Ÿæˆå‡½æ•°
    """
    if seed is not None:
        torch.manual_seed(seed)
    
    # åŠ è½½æ¨¡å‹
    device = "mps" if device == "auto" and hasattr(torch.backends, "mps") and torch.backends.mps.is_available() else device
    torch_dtype = torch.float16 if dtype_str == "auto" and device == "mps" else torch.float32
    
    try:
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_id, torch_dtype=torch_dtype, device_map="auto"
        )
        processor = AutoProcessor.from_pretrained(model_id)
    except:
        from transformers import AutoModelForCausalLM, AutoProcessor
        model = AutoModelForCausalLM.from_pretrained(
            model_id, torch_dtype=torch_dtype, device_map="auto", trust_remote_code=True
        )
        processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    
    # åˆå§‹åŒ–æ··åˆæ¨¡å‹
    cos_vot_model = CoSVoTModel(model, processor, device)
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    
    # æ‰§è¡Œå¯è§†åŒ–äº¤äº’å¼æ¨ç†
    response = cos_vot_model.visual_interactive_reasoning(image, question)
    
    # ä¿å­˜å¯è§†åŒ–ç»“æœ
    if save_visualization:
        # åˆ›å»ºå¯è§†åŒ–å›¾åƒ
        viz_image = cos_vot_model.image_cropper.visualize_roi(image, response.final_roi)
        viz_path = f"{output_dir}/cos_vot_visualization.png"
        viz_image.save(viz_path)
        
        # ä¿å­˜ç©ºé—´å¯è§†åŒ–æ–‡æœ¬
        viz_text_path = f"{output_dir}/spatial_visualization.txt"
        with open(viz_text_path, 'w', encoding='utf-8') as f:
            f.write(response.spatial_visualization)
    
    # æ„å»ºè¿”å›ç»“æœ
    result = {
        "method": "CoS+VoTæ··åˆæ–¹æ³•",
        "question": question,
        "image_path": image_path,
        "final_roi": response.final_roi.to_string(),
        "final_answer": response.final_answer,
        "confidence": response.confidence,
        "roi_steps": len(response.visual_trajectory),
        "visual_trajectory": [
            {
                "step": roi.step_id,
                "bbox": roi.bbox.to_string(),
                "confidence": roi.confidence,
                "visualization": roi.visualization,
                "reasoning": roi.reasoning
            }
            for roi in response.visual_trajectory
        ],
        "reasoning_trace": response.reasoning_trace,
        "spatial_visualization": response.spatial_visualization,
        "innovation_features": [
            "å¤šæ­¥éª¤ROIç»†åŒ–",
            "ç©ºé—´å¯è§†åŒ–æ¨ç†",
            "åŠ¨æ€ROIè°ƒæ•´",
            "ç½®ä¿¡åº¦åŠ æƒå¹³å‡",
            "å¯è§†åŒ–è½¨è¿¹è®°å½•"
        ]
    }
    
    return result


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="CoS+VoTæ··åˆæ–¹æ³•æµ‹è¯•")
    parser.add_argument("--image", required=True, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--question", required=True, help="é—®é¢˜")
    parser.add_argument("--device", default="auto", help="è®¾å¤‡")
    parser.add_argument("--dtype", default="auto", help="æ•°æ®ç±»å‹")
    parser.add_argument("--max-roi-steps", type=int, default=3, help="æœ€å¤§ROIæ­¥éª¤æ•°")
    parser.add_argument("--save-viz", action="store_true", help="ä¿å­˜å¯è§†åŒ–")
    
    args = parser.parse_args()
    
    result = cos_vot_generate(
        model_id="Qwen/Qwen2.5-VL-3B-Instruct",
        image_path=args.image,
        question=args.question,
        device=args.device,
        dtype_str=args.dtype,
        max_roi_steps=args.max_roi_steps,
        save_visualization=args.save_viz
    )
    
    print("=" * 80)
    print("ğŸ”¬ CoS+VoT æ··åˆæ–¹æ³•ç»“æœ")
    print("=" * 80)
    print(f"é—®é¢˜: {result['question']}")
    print(f"æœ€ç»ˆROI: {result['final_roi']}")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
    print(f"ROIæ­¥éª¤æ•°: {result['roi_steps']}")
    print(f"æœ€ç»ˆç­”æ¡ˆ: {result['final_answer']}")
    
    print("\nğŸ“Š å¯è§†åŒ–è½¨è¿¹:")
    for step in result['visual_trajectory']:
        print(f"æ­¥éª¤{step['step']}: ROI={step['bbox']}, ç½®ä¿¡åº¦={step['confidence']:.3f}")
    
    print("\nğŸ¨ ç©ºé—´å¯è§†åŒ–:")
    print(result['spatial_visualization'])
    
    print("\nâœ¨ åˆ›æ–°ç‰¹æ€§:")
    for feature in result['innovation_features']:
        print(f"- {feature}")
