#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦MCOT + Chain-of-Spotç»“åˆç‰ˆæœ¬

é›†æˆYOLOç›®æ ‡æ£€æµ‹ + åŒºåŸŸæ ‡æ³¨ + Basic CoTæ€ç»´é“¾æ¨ç†
"""

import torch
import numpy as np
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass
from PIL import Image, ImageDraw
import re
import json
import time
import cv2
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
    HAS_NATIVE_QWEN25_VL = True
except ImportError:
    from transformers import AutoModelForCausalLM, AutoProcessor
    Qwen2_5_VLForConditionalGeneration = None
    HAS_NATIVE_QWEN25_VL = False


@dataclass
class DetectedRegion:
    """æ£€æµ‹åˆ°çš„åŒºåŸŸ"""
    bbox: List[float]  # [x0, y0, x1, y1] å½’ä¸€åŒ–åæ ‡
    confidence: float
    label: str
    content: str
    region_id: int


@dataclass
class DeepMCotCoSResponse:
    """æ·±åº¦MCOT+CoSå“åº”"""
    detected_regions: List[DetectedRegion]
    region_annotations: List[str]
    cot_reasoning_steps: List[str]
    final_answer: str
    confidence: float
    detection_time: float
    annotation_time: float
    cot_time: float
    total_time: float


class ObjectDetector:
    """ç›®æ ‡æ£€æµ‹å™¨"""
    
    def __init__(self, device: str = "cpu"):
        self.device = device
        self.model = None
        self.initialized = False
    
    def initialize_detector(self, model_type: str = "yolo"):
        """åˆå§‹åŒ–æ£€æµ‹å™¨"""
        try:
            if model_type == "yolo":
                try:
                    from ultralytics import YOLO
                    # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æƒé‡
                    local_weight = os.path.join(os.path.dirname(__file__), '..', '..', 'scripts', 'yolo', 'yolov8n.pt')
                    local_weight = os.path.abspath(local_weight)
                    if os.path.exists(local_weight):
                        self.model = YOLO(local_weight)
                        print(f"âœ… YOLOv8æ£€æµ‹å™¨åŠ è½½æœ¬åœ°æƒé‡: {local_weight}")
                    else:
                        self.model = YOLO('yolov8n.pt')
                        print("âœ… YOLOv8æ£€æµ‹å™¨åŠ è½½é»˜è®¤æƒé‡ yolov8n.pt")
                    self.initialized = True
                    print("âœ… YOLOv8æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
                except ImportError:
                    print("âš ï¸ æœªå®‰è£…ultralyticsï¼Œä½¿ç”¨è¾¹ç¼˜æ£€æµ‹")
                    self._init_edge_detector()
            else:
                self._init_edge_detector()
        except Exception as e:
            print(f"âŒ æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.initialized = False
    
    def _init_edge_detector(self):
        """åˆå§‹åŒ–è¾¹ç¼˜æ£€æµ‹å™¨"""
        self.model = "edge_detection"
        self.initialized = True
        print("âœ… è¾¹ç¼˜æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def detect_regions(self, image: Image.Image, min_confidence: float = 0.3) -> List[DetectedRegion]:
        """æ£€æµ‹å›¾åƒä¸­çš„åŒºåŸŸ"""
        if not self.initialized:
            self.initialize_detector()
        
        if isinstance(self.model, str) and self.model == "edge_detection":
            return self._edge_detection(image)
        elif hasattr(self.model, 'predict'):
            return self._yolo_detection(image, min_confidence)
        else:
            return self._edge_detection(image)
    
    def _edge_detection(self, image: Image.Image) -> List[DetectedRegion]:
        """è¾¹ç¼˜æ£€æµ‹æ–¹æ³•"""
        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        regions = []
        img_height, img_width = gray.shape
        
        for i, contour in enumerate(contours):
            x, y, w, h = cv2.boundingRect(contour)
            if w < 20 or h < 20:
                continue
            
            x0 = x / img_width
            y0 = y / img_height
            x1 = (x + w) / img_width
            y1 = (y + h) / img_height
            
            area = w * h
            confidence = min(0.8, area / (img_width * img_height) * 10)
            
            regions.append(DetectedRegion(
                bbox=[x0, y0, x1, y1],
                confidence=confidence,
                label=f"region_{i}",
                content="",
                region_id=i
            ))
        
        return regions[:5]
    
    def _yolo_detection(self, image: Image.Image, min_confidence: float) -> List[DetectedRegion]:
        """YOLOæ£€æµ‹æ–¹æ³•"""
        try:
            results = self.model(image)
            regions = []
            
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for i, box in enumerate(boxes):
                        confidence = float(box.conf[0])
                        if confidence < min_confidence:
                            continue
                        
                        x0, y0, x1, y1 = box.xyxy[0].cpu().numpy()
                        img_width, img_height = image.size
                        
                        x0_norm = x0 / img_width
                        y0_norm = y0 / img_height
                        x1_norm = x1 / img_width
                        y1_norm = y1 / img_height
                        
                        class_id = int(box.cls[0])
                        label = f"object_{class_id}"
                        
                        regions.append(DetectedRegion(
                            bbox=[x0_norm, y0_norm, x1_norm, y1_norm],
                            confidence=confidence,
                            label=label,
                            content="",
                            region_id=i
                        ))
            
            return regions
        except Exception as e:
            print(f"YOLOæ£€æµ‹å¤±è´¥: {e}")
            return self._edge_detection(image)


class RegionAnnotator:
    """åŒºåŸŸæ ‡æ³¨å™¨"""
    
    def __init__(self, model, processor, device: str):
        self.model = model
        self.processor = processor
        self.device = device
    
    def annotate_regions(self, image: Image.Image, regions: List[DetectedRegion]) -> List[str]:
        """æ ‡æ³¨æ£€æµ‹åˆ°çš„åŒºåŸŸï¼Œè¿”å›æ ‡æ³¨æ–‡æœ¬åˆ—è¡¨"""
        annotations = []
        
        for region in regions:
            x0, y0, x1, y1 = region.bbox
            img_width, img_height = image.size
            
            x0_pixel = max(0, min(int(x0 * img_width), img_width))
            y0_pixel = max(0, min(int(y0 * img_height), img_height))
            x1_pixel = max(x0_pixel, min(int(x1 * img_width), img_width))
            y1_pixel = max(y0_pixel, min(int(y1 * img_height), img_height))
            
            if x1_pixel <= x0_pixel or y1_pixel <= y0_pixel:
                continue
            
            try:
                region_image = image.crop((x0_pixel, y0_pixel, x1_pixel, y1_pixel))
                content = self._annotate_single_region(region_image, region.label)
                annotations.append(f"åŒºåŸŸ{region.region_id}: {content}")
            except Exception as e:
                print(f"åŒºåŸŸæ ‡æ³¨å¤±è´¥: {e}")
                annotations.append(f"åŒºåŸŸ{region.region_id}: æ ‡æ³¨å¤±è´¥")
        
        return annotations
    
    def _annotate_single_region(self, region_image: Image.Image, label: str) -> str:
        """æ ‡æ³¨å•ä¸ªåŒºåŸŸ"""
        try:
            prompt = f"è¯·æè¿°è¿™ä¸ªå›¾åƒåŒºåŸŸä¸­çš„å†…å®¹ï¼Œç®€æ´æ˜äº†ï¼š"
            
            messages = [{
                "role": "user",
                "content": [
                    {"type": "image", "image": region_image},
                    {"type": "text", "text": prompt}
                ]
            }]
            
            try:
                chat_text = self.processor.apply_chat_template(
                    messages=messages, tokenize=False, add_generation_prompt=True
                )
            except Exception:
                chat_text = self.processor.apply_chat_template(
                    conversation=messages, tokenize=False, add_generation_prompt=True
                )
            
            inputs = self.processor(
                text=[chat_text],
                images=[region_image],
                padding=True,
                return_tensors="pt",
            )
            
            if self.device in ("cuda", "mps", "npu", "xpu"):
                inputs = {k: v.to(self.device) if hasattr(v, "to") else v for k, v in inputs.items()}
            
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=False,
                    temperature=0.1,
                )
            
            try:
                trimmed = [out[len(inp):] for inp, out in zip(inputs["input_ids"], generated_ids)]
                content = self.processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            except Exception:
                content = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
            
            return content.strip()
            
        except Exception as e:
            print(f"åŒºåŸŸæ ‡æ³¨å¤±è´¥: {e}")
            return f"åŒºåŸŸå†…å®¹: {label}"


class BasicCoTReasoner:
    """Basic CoTæ¨ç†å™¨"""
    
    def __init__(self, model, processor, device: str):
        self.model = model
        self.processor = processor
        self.device = device
    
    def cot_reasoning(self, image: Image.Image, question: str, region_annotations: List[str]) -> List[str]:
        """æ‰§è¡ŒCoTæ¨ç†ï¼Œè¿”å›æ¨ç†æ­¥éª¤"""
        
        # æ„å»ºCoTæç¤º
        region_text = "\n".join(region_annotations) if region_annotations else "æœªæ£€æµ‹åˆ°æ˜æ˜¾åŒºåŸŸ"
        
        cot_prompt = f"""è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œé€æ­¥æ¨ç†æ¥å›ç­”é—®é¢˜ï¼š

æ£€æµ‹åˆ°çš„åŒºåŸŸä¿¡æ¯ï¼š
{region_text}

é—®é¢˜ï¼š{question}

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œæ¨ç†ï¼š
1. é¦–å…ˆåˆ†æå›¾åƒä¸­çš„ä¸»è¦å†…å®¹å’ŒåŒºåŸŸ
2. ç„¶åæ ¹æ®é—®é¢˜è¦æ±‚ï¼Œåˆ†æç›¸å…³åŒºåŸŸçš„ç‰¹å¾
3. æœ€åç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç»™å‡ºè¯¦ç»†ç­”æ¡ˆ

è¯·é€æ­¥æ€è€ƒï¼š"""

        messages = [{
            "role": "user",
            "content": [
                {"type": "image", "image": image},
                {"type": "text", "text": cot_prompt}
            ]
        }]
        
        try:
            chat_text = self.processor.apply_chat_template(
                messages=messages, tokenize=False, add_generation_prompt=True
            )
        except Exception:
            chat_text = self.processor.apply_chat_template(
                conversation=messages, tokenize=False, add_generation_prompt=True
            )
        
        inputs = self.processor(
            text=[chat_text],
            images=[image],
            padding=True,
            return_tensors="pt",
        )
        
        if self.device in ("cuda", "mps", "npu", "xpu"):
            inputs = {k: v.to(self.device) if hasattr(v, "to") else v for k, v in inputs.items()}
        
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=1024,
                do_sample=True,
                temperature=0.3,
                top_p=0.9,
            )
        
        try:
            trimmed = [out[len(inp):] for inp, out in zip(inputs["input_ids"], generated_ids)]
            reasoning_text = self.processor.batch_decode(trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        except Exception:
            reasoning_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        
        # è§£ææ¨ç†æ­¥éª¤
        reasoning_steps = self._parse_reasoning_steps(reasoning_text)
        return reasoning_steps
    
    def _parse_reasoning_steps(self, reasoning_text: str) -> List[str]:
        """è§£ææ¨ç†æ–‡æœ¬ä¸ºæ­¥éª¤åˆ—è¡¨"""
        steps = []
        
        # å°è¯•æŒ‰æ•°å­—ç¼–å·åˆ†å‰²
        lines = reasoning_text.split('\n')
        current_step = ""
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„æ­¥éª¤ï¼ˆä»¥æ•°å­—å¼€å¤´ï¼‰
            if re.match(r'^\d+[\.\)]', line):
                if current_step:
                    steps.append(current_step.strip())
                current_step = line
            else:
                current_step += " " + line
        
        if current_step:
            steps.append(current_step.strip())
        
        # å¦‚æœæ²¡æœ‰è§£æåˆ°æ­¥éª¤ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªæ­¥éª¤
        if not steps:
            steps = [reasoning_text.strip()]
        
        return steps


class DeepMCotCoSModel:
    """æ·±åº¦MCOT+CoSæ¨¡å‹"""
    
    def __init__(self, model_id: str, device: str = "auto", detector_type: str = "yolo"):
        self.device = self._auto_select_device(device)
        self.detector_type = detector_type
        
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        self.object_detector = ObjectDetector(self.device)
        self.object_detector.initialize_detector(detector_type)
        
        # åŠ è½½æ¨¡å‹
        self.model, self.processor = self._load_model(model_id)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.region_annotator = RegionAnnotator(self.model, self.processor, self.device)
        self.cot_reasoner = BasicCoTReasoner(self.model, self.processor, self.device)
    
    def _auto_select_device(self, device: str) -> str:
        """è‡ªåŠ¨é€‰æ‹©è®¾å¤‡"""
        if device != "auto":
            return device
        
        if hasattr(torch, 'npu') and torch.npu.is_available():
            return "npu"
        elif torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch, 'xpu') and torch.xpu.is_available():
            return "xpu"
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    def _load_model(self, model_id: str):
        """åŠ è½½æ¨¡å‹"""
        torch_dtype = torch.float16 if self.device in ("npu", "mps", "xpu") else torch.float32
        
        if HAS_NATIVE_QWEN25_VL and Qwen2_5_VLForConditionalGeneration is not None:
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id,
                torch_dtype=torch_dtype,
                device_map="auto" if self.device in ("cuda", "mps", "npu", "xpu") else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch_dtype,
                device_map="auto" if self.device in ("cuda", "mps", "npu", "xpu") else None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
        
        return model, processor
    
    def deep_reasoning(self, image: Image.Image, question: str) -> DeepMCotCoSResponse:
        """æ·±åº¦æ¨ç†æµç¨‹"""
        total_start = time.time()
        
        print("ğŸ”¬ å¼€å§‹æ·±åº¦MCOT+CoSæ¨ç†...")
        
        # æ­¥éª¤1: YOLOç›®æ ‡æ£€æµ‹
        print("ğŸ“Š æ­¥éª¤1: YOLOç›®æ ‡æ£€æµ‹")
        detection_start = time.time()
        detected_regions = self.object_detector.detect_regions(image, min_confidence=0.3)
        detection_time = time.time() - detection_start
        
        print(f"âœ… æ£€æµ‹å®Œæˆ: {len(detected_regions)} ä¸ªåŒºåŸŸï¼Œè€—æ—¶ {detection_time:.2f}ç§’")
        
        # å¯è§†åŒ–å¹¶ä¿å­˜æ£€æµ‹ç»“æœ
        try:
            out_dir = "/test_res"
            os.makedirs(out_dir, exist_ok=True)
            viz_img = _create_visualization(image, detected_regions)
            out_path = os.path.join(out_dir, f"deep_mcot_detection_{self.detector_type}.png")
            viz_img.save(out_path)
            print(f"âœ… æ£€æµ‹å¯è§†åŒ–å·²ä¿å­˜: {out_path}")
        except Exception as e:
            print(f"âš ï¸ æ£€æµ‹å¯è§†åŒ–ä¿å­˜å¤±è´¥: {e}")
        
        # è¾“å‡ºæ£€æµ‹ç»“æœè¯¦æƒ…
        if detected_regions:
            print("ğŸ“‹ æ£€æµ‹ç»“æœæ˜ç»†:")
            for r in detected_regions:
                print(f"  - id={r.region_id}, label={r.label}, conf={r.confidence:.3f}, bbox={r.bbox}")
        else:
            print("ğŸ“‹ æ£€æµ‹ç»“æœæ˜ç»†: æ— æ£€æµ‹æ¡†")
        
        # æ­¥éª¤2: åŒºåŸŸæ ‡æ³¨
        print("\nğŸ“ æ­¥éª¤2: åŒºåŸŸæ ‡æ³¨")
        annotation_start = time.time()
        region_annotations = self.region_annotator.annotate_regions(image, detected_regions)
        annotation_time = time.time() - annotation_start
        
        print(f"âœ… æ ‡æ³¨å®Œæˆ: {len(region_annotations)} ä¸ªåŒºåŸŸï¼Œè€—æ—¶ {annotation_time:.2f}ç§’")
        for i, annotation in enumerate(region_annotations):
            print(f"  {annotation}")
        
        # æ­¥éª¤3: Basic CoTæ¨ç†
        print("\nğŸ§  æ­¥éª¤3: Basic CoTæ¨ç†")
        cot_start = time.time()
        cot_steps = self.cot_reasoner.cot_reasoning(image, question, region_annotations)
        cot_time = time.time() - cot_start
        
        print(f"âœ… CoTæ¨ç†å®Œæˆ: {len(cot_steps)} ä¸ªæ­¥éª¤ï¼Œè€—æ—¶ {cot_time:.2f}ç§’")
        
        # æå–æœ€ç»ˆç­”æ¡ˆï¼ˆæœ€åä¸€ä¸ªæ­¥éª¤æˆ–æ•´ä¸ªæ¨ç†æ–‡æœ¬ï¼‰
        final_answer = cot_steps[-1] if cot_steps else "æ¨ç†å¤±è´¥"
        
        total_time = time.time() - total_start
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = np.mean([region.confidence for region in detected_regions]) if detected_regions else 0.5
        
        print(f"\nğŸ¯ æ·±åº¦æ¨ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return DeepMCotCoSResponse(
            detected_regions=detected_regions,
            region_annotations=region_annotations,
            cot_reasoning_steps=cot_steps,
            final_answer=final_answer,
            confidence=confidence,
            detection_time=detection_time,
            annotation_time=annotation_time,
            cot_time=cot_time,
            total_time=total_time
        )


def _create_visualization(image: Image.Image, regions: List[DetectedRegion]) -> Image.Image:
    """åˆ›å»ºå¯è§†åŒ–ç»“æœ"""
    viz_image = image.copy()
    draw = ImageDraw.Draw(viz_image)
    
    colors = ["red", "blue", "green", "yellow", "purple", "orange"]
    
    for i, region in enumerate(regions):
        color = colors[i % len(colors)]
        x0, y0, x1, y1 = region.bbox
        
        img_width, img_height = image.size
        x0_pixel = int(x0 * img_width)
        y0_pixel = int(y0 * img_height)
        x1_pixel = int(x1 * img_width)
        y1_pixel = int(y1 * img_height)
        
        draw.rectangle([x0_pixel, y0_pixel, x1_pixel, y1_pixel], outline=color, width=3)
        
        label = f"R{region.region_id}: {region.label}"
        draw.text((x0_pixel, y0_pixel - 20), label, fill=color)
    
    return viz_image


def deep_mcot_cos_generate(
    model_id: str,
    image_path: str,
    question: str,
    device: str = "auto",
    detector_type: str = "yolo",
    save_visualization: bool = False,
    output_dir: str = ".",
) -> Dict[str, Any]:
    """æ·±åº¦MCOT+CoSç”Ÿæˆå‡½æ•°"""
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    
    # åˆå§‹åŒ–æ·±åº¦MCOT+CoSæ¨¡å‹
    deep_model = DeepMCotCoSModel(model_id, device, detector_type)
    
    # æ‰§è¡Œæ·±åº¦æ¨ç†
    response = deep_model.deep_reasoning(image, question)
    
    # ä¿å­˜å¯è§†åŒ–ç»“æœ
    if save_visualization:
        viz_image = _create_visualization(image, response.detected_regions)
        viz_path = f"{output_dir}/deep_mcot_cos_visualization.png"
        viz_image.save(viz_path)
    
    # æ„å»ºè¿”å›ç»“æœ
    result = {
        "method": "Deep MCOT + Chain-of-Spot",
        "device": device,
        "detector_type": detector_type,
        "question": question,
        "image_path": image_path,
        "detected_regions": [
            {
                "region_id": region.region_id,
                "bbox": region.bbox,
                "confidence": region.confidence,
                "label": region.label,
                "content": region.content
            }
            for region in response.detected_regions
        ],
        "region_annotations": response.region_annotations,
        "cot_reasoning_steps": response.cot_reasoning_steps,
        "final_answer": response.final_answer,
        "confidence": response.confidence,
        "timing": {
            "detection_time": response.detection_time,
            "annotation_time": response.annotation_time,
            "cot_time": response.cot_time,
            "total_time": response.total_time
        },
        "improvements": [
            "YOLOv8ç›®æ ‡æ£€æµ‹",
            "åŒºåŸŸçº§å†…å®¹æ ‡æ³¨",
            "Basic CoTæ€ç»´é“¾æ¨ç†",
            "å¤šæ­¥éª¤æ·±åº¦åˆ†æ",
            "å¯è§†åŒ–æ£€æµ‹ç»“æœ"
        ]
    }
    
    return result


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æ·±åº¦MCOT+Chain-of-Spotæ¨ç†")
    parser.add_argument("--model-id", type=str, default="Qwen/Qwen2.5-VL-3B-Instruct")
    parser.add_argument("--image", type=str, required=True, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--question", type=str, required=True, help="é—®é¢˜")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡")
    parser.add_argument("--detector", type=str, default="yolo", choices=["yolo", "edge"], help="æ£€æµ‹å™¨ç±»å‹")
    parser.add_argument("--save-viz", action="store_true", help="ä¿å­˜å¯è§†åŒ–ç»“æœ")
    
    args = parser.parse_args()
    
    result = deep_mcot_cos_generate(
        model_id=args.model_id,
        image_path=args.image,
        question=args.question,
        device=args.device,
        detector_type=args.detector,
        save_visualization=args.save_viz
    )
    
    print("\n" + "=" * 80)
    print("ğŸ§  æ·±åº¦MCOT+Chain-of-Spotç»“æœ")
    print("=" * 80)
    print(f"è®¾å¤‡: {result['device']}")
    print(f"æ£€æµ‹å™¨: {result['detector_type']}")
    print(f"é—®é¢˜: {result['question']}")
    print(f"æ£€æµ‹åŒºåŸŸæ•°: {len(result['detected_regions'])}")
    print(f"æ£€æµ‹è€—æ—¶: {result['timing']['detection_time']:.2f}ç§’")
    print(f"æ ‡æ³¨è€—æ—¶: {result['timing']['annotation_time']:.2f}ç§’")
    print(f"CoTæ¨ç†è€—æ—¶: {result['timing']['cot_time']:.2f}ç§’")
    print(f"æ€»è€—æ—¶: {result['timing']['total_time']:.2f}ç§’")
    print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
    
    print(f"\nğŸ“ åŒºåŸŸæ ‡æ³¨:")
    for annotation in result['region_annotations']:
        print(f"  {annotation}")
    
    print(f"\nğŸ§  CoTæ¨ç†æ­¥éª¤:")
    for i, step in enumerate(result['cot_reasoning_steps'], 1):
        print(f"  æ­¥éª¤{i}: {step}")
    
    print(f"\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆ: {result['final_answer']}")
    
    print("\nâœ¨ æ”¹è¿›ç‰¹æ€§:")
    for improvement in result['improvements']:
        print(f"  - {improvement}")
