# ğŸ§  NPUç¯å¢ƒé…ç½®æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨åä¸ºæ˜‡è…¾NPUä¸Šè¿è¡ŒChain-of-Spotå’ŒVoTæ··åˆæ–¹æ³•ã€‚

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- åä¸ºæ˜‡è…¾910/910B NPU
- è‡³å°‘16GB NPUå†…å­˜
- æ”¯æŒNPUçš„ä¸»æ¿

### è½¯ä»¶è¦æ±‚
- CANN (Compute Architecture for Neural Networks) 7.0+
- PyTorch 2.0+ with NPU support
- Python 3.8+

## ğŸ”§ ç¯å¢ƒå®‰è£…

### 1. å®‰è£…CANNå·¥å…·åŒ…

```bash
# ä¸‹è½½CANNå·¥å…·åŒ…
wget https://download.huawei.com/download/ascend/software/cann/7.0.0/linux/x86_64/Ascend-cann-toolkit_7.0.0_linux-x86_64.run

# å®‰è£…CANN
chmod +x Ascend-cann-toolkit_7.0.0_linux-x86_64.run
./Ascend-cann-toolkit_7.0.0_linux-x86_64.run --install
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

```bash
# æ·»åŠ åˆ° ~/.bashrc
export ASCEND_HOME=/usr/local/Ascend
export PATH=${ASCEND_HOME}/ascend-toolkit/latest/compiler/ccec_linux/bin:${ASCEND_HOME}/ascend-toolkit/latest/compiler/bin:${PATH}
export PATH=${ASCEND_HOME}/ascend-toolkit/latest/fwkacllib/ccec_linux/bin:${ASCEND_HOME}/ascend-toolkit/latest/fwkacllib/bin:${PATH}
export ASCEND_OPP_PATH=${ASCEND_HOME}/ascend-toolkit/latest/opp
export TOOLCHAIN_HOME=${ASCEND_HOME}/ascend-toolkit/latest/toolkit
export ASCEND_SLOG_PRINT_TO_STDOUT=1
export ASCEND_GLOBAL_LOG_LEVEL=3
export ASCEND_SLOG_PRINT_TO_STDOUT=0
export TBE_IMPL_PATH=${ASCEND_HOME}/ascend-toolkit/latest/opp/op_impl/built-in/ai_core/tbe
export ASCEND_AICPU_PATH=${ASCEND_HOME}/ascend-toolkit/latest
export ASCEND_TENSOR_COMPILER_INCLUDE=${ASCEND_HOME}/ascend-toolkit/latest/compiler/ccec_linux/include
export ASCEND_TENSOR_COMPILER_LIBRARY_PATH=${ASCEND_HOME}/ascend-toolkit/latest/compiler/ccec_linux/lib64
export ASCEND_TENSOR_COMPILER_OPP_PATH=${ASCEND_HOME}/ascend-toolkit/latest/opp
export TOOLCHAIN_HOME=${ASCEND_HOME}/ascend-toolkit/latest/toolkit
export ASCEND_DEVICE_ID=0
export ASCEND_VISIBLE_DEVICES=0
```

### 3. å®‰è£…PyTorch NPUç‰ˆæœ¬

```bash
# å®‰è£…torch_npu
pip install torch_npu

# éªŒè¯å®‰è£…
python -c "import torch; print(torch.npu.is_available())"
```

### 4. å®‰è£…å…¶ä»–ä¾èµ–

```bash
pip install transformers pillow numpy
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```bash
# ä½¿ç”¨NPUè¿è¡ŒCoS+VoT
python cos_vot_npu.py --image test_image.jpg --question "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ" --device npu

# æŒ‡å®šæ•°æ®ç±»å‹
python cos_vot_npu.py --image test_image.jpg --question "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ" --device npu --dtype fp16

# ä¿å­˜å¯è§†åŒ–ç»“æœ
python cos_vot_npu.py --image test_image.jpg --question "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ" --device npu --save-viz
```

### é«˜çº§é…ç½®

```bash
# è‡ªå®šä¹‰ROIæ­¥éª¤æ•°
python cos_vot_npu.py --image test_image.jpg --question "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ" --device npu --max-roi-steps 5

# ä½¿ç”¨ä¸åŒç²¾åº¦
python cos_vot_npu.py --image test_image.jpg --question "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ" --device npu --dtype bf16
```

## âš¡ NPUä¼˜åŒ–ç‰¹æ€§

### 1. å›¾ä¼˜åŒ–
- è‡ªåŠ¨èåˆç®—å­
- å‡å°‘å†…å­˜æ‹·è´
- ä¼˜åŒ–è®¡ç®—å›¾

### 2. å†…å­˜ç®¡ç†
- æ™ºèƒ½å†…å­˜åˆ†é…
- è‡ªåŠ¨ç¼“å­˜æ¸…ç†
- å†…å­˜ç¢ç‰‡æ•´ç†

### 3. ç²¾åº¦ä¼˜åŒ–
- FP16è‡ªåŠ¨æ··åˆç²¾åº¦
- åŠ¨æ€ç²¾åº¦è°ƒæ•´
- é‡åŒ–æ”¯æŒ

### 4. å¹¶è¡Œä¼˜åŒ–
- å¤šNPUå¹¶è¡Œ
- æµæ°´çº¿å¹¶è¡Œ
- æ•°æ®å¹¶è¡Œ

## ğŸ” æ€§èƒ½ç›‘æ§

### æŸ¥çœ‹NPUä½¿ç”¨æƒ…å†µ

```bash
# æŸ¥çœ‹NPUè®¾å¤‡çŠ¶æ€
npu-smi info

# ç›‘æ§NPUä½¿ç”¨ç‡
npu-smi monitor -i 0 -c 10

# æŸ¥çœ‹è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
npu-smi info -t board -i 0
```

### æ€§èƒ½è°ƒä¼˜å‚æ•°

```python
# åœ¨ä»£ç ä¸­è®¾ç½®æ€§èƒ½å‚æ•°
import torch_npu

# è®¾ç½®NPUæ€§èƒ½æ¨¡å¼
torch_npu.npu.set_perf_mode(True)

# è®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
torch_npu.npu.set_memory_fraction(0.8)

# å¯ç”¨å›¾ä¼˜åŒ–
torch_npu.npu.set_graph_mode(True)
```

## ğŸ› ï¸ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **NPUä¸å¯ç”¨**
   ```bash
   # æ£€æŸ¥NPUé©±åŠ¨
   lspci | grep -i ascend
   
   # æ£€æŸ¥CANNå®‰è£…
   ls /usr/local/Ascend/ascend-toolkit/latest/
   ```

2. **å†…å­˜ä¸è¶³**
   ```bash
   # æ¸…ç†NPUç¼“å­˜
   python -c "import torch_npu; torch_npu.npu.empty_cache()"
   
   # å‡å°‘batch sizeæˆ–æ¨¡å‹ç²¾åº¦
   ```

3. **æ€§èƒ½é—®é¢˜**
   ```bash
   # æ£€æŸ¥NPUé¢‘ç‡
   npu-smi info -t board -i 0
   
   # è°ƒæ•´æ€§èƒ½æ¨¡å¼
   npu-smi set -i 0 -c 0 -t performance
   ```

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export ASCEND_GLOBAL_LOG_LEVEL=0
export ASCEND_SLOG_PRINT_TO_STDOUT=1

# è¿è¡Œç¨‹åº
python cos_vot_npu.py --image test_image.jpg --question "å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆï¼Ÿ" --device npu
```

## ğŸ“Š æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ
- ç¡¬ä»¶: åä¸ºæ˜‡è…¾910
- æ¨¡å‹: Qwen2.5-VL-3B-Instruct
- å›¾åƒ: 512x512

### æ€§èƒ½æ•°æ®

| æ–¹æ³• | è®¾å¤‡ | æ¨ç†æ—¶é—´ | å†…å­˜ä½¿ç”¨ | ç²¾åº¦ |
|------|------|----------|----------|------|
| æ ‡å‡†CoS | CPU | 15.2s | 8GB | FP32 |
| æ ‡å‡†CoS | GPU | 3.8s | 6GB | FP16 |
| CoS+VoT | NPU | 2.1s | 4GB | FP16 |

## ğŸ”„ è¿ç§»æŒ‡å—

### ä»GPUè¿ç§»åˆ°NPU

1. **ä¿®æ”¹è®¾å¤‡é€‰æ‹©**
   ```python
   # åŸGPUä»£ç 
   device = "cuda" if torch.cuda.is_available() else "cpu"
   
   # NPUä»£ç 
   device = "npu" if torch.npu.is_available() else "cpu"
   ```

2. **ä¿®æ”¹æ¨¡å‹åŠ è½½**
   ```python
   # åŸGPUä»£ç 
   model = model.to("cuda")
   
   # NPUä»£ç 
   model = model.to("npu")
   model = torch_npu.optimize(model)  # å¯ç”¨å›¾ä¼˜åŒ–
   ```

3. **ä¿®æ”¹æ•°æ®ç±»å‹**
   ```python
   # NPUæ¨èä½¿ç”¨FP16
   torch_dtype = torch.float16
   ```

### ä»CPUè¿ç§»åˆ°NPU

1. **å®‰è£…NPUç¯å¢ƒ**
2. **ä¿®æ”¹è®¾å¤‡æ£€æµ‹é€»è¾‘**
3. **ä¼˜åŒ–å†…å­˜ä½¿ç”¨**
4. **è°ƒæ•´æ‰¹å¤„ç†å¤§å°**

## ğŸ“š å‚è€ƒèµ„æ–™

- [åä¸ºæ˜‡è…¾å®˜æ–¹æ–‡æ¡£](https://www.hiascend.com/software/cann)
- [PyTorch NPUæ–‡æ¡£](https://gitee.com/ascend/pytorch)
- [CANNå¼€å‘è€…æŒ‡å—](https://www.hiascend.com/document)

## ğŸ¤ æŠ€æœ¯æ”¯æŒ

å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š
1. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„æ•…éšœæ’é™¤éƒ¨åˆ†
2. æ£€æŸ¥åä¸ºæ˜‡è…¾å®˜æ–¹è®ºå›
3. è”ç³»æŠ€æœ¯æ”¯æŒå›¢é˜Ÿ
