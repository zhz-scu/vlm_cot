#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Chain-of-Spot + VoT æ··åˆæ–¹æ³• - NPUä¸“ç”¨ç‰ˆæœ¬

ä¸“é—¨ä¸ºåä¸ºæ˜‡è…¾NPUä¼˜åŒ–çš„ç‰ˆæœ¬ï¼ŒåŒ…å«ï¼š
1. NPUè®¾å¤‡æ£€æµ‹å’Œé…ç½®
2. NPUç‰¹å®šçš„æ¨¡å‹åŠ è½½
3. NPUå†…å­˜ä¼˜åŒ–
4. NPUæ€§èƒ½è°ƒä¼˜
"""

import torch
import numpy as np
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass
from PIL import Image, ImageDraw, ImageFont
import re
import json
import argparse
import sys
import time

# NPUç›¸å…³å¯¼å…¥
try:
    import torch_npu
    HAS_NPU = True
except ImportError:
    HAS_NPU = False
    print("è­¦å‘Š: æœªå®‰è£…torch_npuï¼Œæ— æ³•ä½¿ç”¨NPUåŠŸèƒ½")

from cos_model import ChainOfSpotModel, BoundingBox, CoSResponse


@dataclass
class VisualROI:
    """å¯è§†åŒ–ROIç»“æ„"""
    bbox: BoundingBox
    confidence: float
    visualization: str
    reasoning: str
    step_id: int


@dataclass
class CoSVoTResponse:
    """CoS+VoTæ··åˆå“åº”"""
    final_roi: BoundingBox
    final_answer: str
    visual_trajectory: List[VisualROI]
    reasoning_trace: List[str]
    confidence: float
    spatial_visualization: str


class NPUOptimizer:
    """NPUä¼˜åŒ–å™¨"""
    
    @staticmethod
    def setup_npu_environment():
        """è®¾ç½®NPUç¯å¢ƒ"""
        if not HAS_NPU:
            raise RuntimeError("NPUç¯å¢ƒæœªé…ç½®ï¼Œè¯·å®‰è£…torch_npu")
        
        # è®¾ç½®NPUç¯å¢ƒå˜é‡
        import os
        os.environ['ASCEND_DEVICE_ID'] = '0'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªNPUè®¾å¤‡
        os.environ['ASCEND_VISIBLE_DEVICES'] = '0'
        
        # åˆå§‹åŒ–NPU
        torch.npu.set_device(0)
        print("NPUç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
    
    @staticmethod
    def optimize_for_npu(model, device="npu"):
        """ä¸ºNPUä¼˜åŒ–æ¨¡å‹"""
        if device != "npu":
            return model
        
        # å¯ç”¨NPUä¼˜åŒ–
        model = model.to(device)
        
        # è®¾ç½®NPUç‰¹å®šçš„ä¼˜åŒ–é€‰é¡¹
        if hasattr(model, 'half'):
            model = model.half()  # ä½¿ç”¨FP16
        
        # å¯ç”¨NPUå›¾ä¼˜åŒ–
        try:
            model = torch.npu.optimize(model)
            print("NPUå›¾ä¼˜åŒ–å·²å¯ç”¨")
        except:
            print("NPUå›¾ä¼˜åŒ–ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†æ¨¡å¼")
        
        return model
    
    @staticmethod
    def clear_npu_cache():
        """æ¸…ç†NPUç¼“å­˜"""
        if HAS_NPU:
            torch.npu.empty_cache()


class SpatialVisualizer:
    """ç©ºé—´å¯è§†åŒ–å™¨"""
    
    def __init__(self, grid_size: int = 8):
        self.grid_size = grid_size
        self.grid_chars = {
            'empty': 'Â·',
            'target': 'â˜…',
            'roi': 'â–ˆ',
            'context': 'â–‘',
            'boundary': 'â”‚'
        }
    
    def create_spatial_grid(self, bbox: BoundingBox, target_desc: str = "ç›®æ ‡") -> str:
        """åˆ›å»ºç©ºé—´ç½‘æ ¼å¯è§†åŒ–"""
        grid = []
        for y in range(self.grid_size):
            row = []
            for x in range(self.grid_size):
                norm_x = x / self.grid_size
                norm_y = y / self.grid_size
                
                if bbox.x0 <= norm_x <= bbox.x1 and bbox.y0 <= norm_y <= bbox.y1:
                    if norm_x == (bbox.x0 + bbox.x1) / 2 and norm_y == (bbox.y0 + bbox.y1) / 2:
                        row.append(self.grid_chars['target'])
                    else:
                        row.append(self.grid_chars['roi'])
                else:
                    row.append(self.grid_chars['empty'])
            grid.append(''.join(row))
        
        result = f"ç©ºé—´å¯è§†åŒ– - {target_desc}:\n"
        result += "â”Œ" + "â”€" * self.grid_size + "â”\n"
        for row in grid:
            result += "â”‚" + row + "â”‚\n"
        result += "â””" + "â”€" * self.grid_size + "â”˜\n"
        result += f"åæ ‡: [{bbox.x0:.2f},{bbox.x1:.2f},{bbox.y0:.2f},{bbox.y1:.2f}]\n"
        
        return result
    
    def create_multi_roi_visualization(self, rois: List[VisualROI]) -> str:
        """åˆ›å»ºå¤šROIå¯è§†åŒ–"""
        grid = []
        for y in range(self.grid_size):
            row = []
            for x in range(self.grid_size):
                norm_x = x / self.grid_size
                norm_y = y / self.grid_size
                
                in_roi = False
                for i, roi in enumerate(rois):
                    if roi.bbox.x0 <= norm_x <= roi.bbox.x1 and roi.bbox.y0 <= norm_y <= roi.bbox.y1:
                        row.append(str(i + 1))
                        in_roi = True
                        break
                
                if not in_roi:
                    row.append(self.grid_chars['empty'])
            grid.append(''.join(row))
        
        result = "å¤šROIæ¼”åŒ–å¯è§†åŒ–:\n"
        result += "â”Œ" + "â”€" * self.grid_size + "â”\n"
        for row in grid:
            result += "â”‚" + row + "â”‚\n"
        result += "â””" + "â”€" * self.grid_size + "â”˜\n"
        
        for i, roi in enumerate(rois):
            result += f"ROI{i+1}: ç½®ä¿¡åº¦={roi.confidence:.2f}, æ­¥éª¤={roi.step_id}\n"
        
        return result


class CoSVoTNPUModel(ChainOfSpotModel):
    """CoS+VoT NPUä¼˜åŒ–æ¨¡å‹"""
    
    def __init__(self, base_model, processor, device: str = "npu"):
        super().__init__(base_model, processor, device)
        self.spatial_visualizer = SpatialVisualizer()
        self.npu_optimizer = NPUOptimizer()
        
        # NPUä¼˜åŒ–
        if device == "npu":
            self.base_model = self.npu_optimizer.optimize_for_npu(self.base_model, device)
        
        # æŒ‡ä»¤æ¨¡æ¿
        self.vot_instruction_1 = (
            "<Img> To answer the question: <Q>, "
            "please identify the region of interest and provide a spatial visualization. "
            "Return coordinates as [x0,x1,y0,y1] and create a text-based spatial grid. "
            "Format: COORDS:[x0,x1,y0,y1] VISUAL:[grid visualization]"
        )
        
        self.vot_instruction_2 = (
            "The region of interest is <ROI Img>. "
            "Based on this focused region and the spatial context, "
            "please provide a detailed answer to: <Q>. "
            "Include spatial reasoning in your response."
        )
    
    def _extract_coords_and_visualization(self, response: str) -> Tuple[Optional[BoundingBox], str]:
        """æå–åæ ‡å’Œå¯è§†åŒ–ä¿¡æ¯"""
        bbox = None
        visualization = ""
        
        # æå–åæ ‡
        coords_match = re.search(r'COORDS:\[([\d.]+),([\d.]+),([\d.]+),([\d.]+)\]', response)
        if coords_match:
            coords = [float(x) for x in coords_match.groups()]
            bbox = BoundingBox(x0=coords[0], x1=coords[1], y0=coords[2], y1=coords[3])
        
        # æå–å¯è§†åŒ–
        visual_match = re.search(r'VISUAL:(.*?)(?=COORDS:|$)', response, re.DOTALL)
        if visual_match:
            visualization = visual_match.group(1).strip()
        
        return bbox, visualization
    
    def _multi_step_roi_refinement(self, image: Image.Image, question: str, 
                                 max_steps: int = 3) -> List[VisualROI]:
        """å¤šæ­¥éª¤ROIç»†åŒ– - NPUä¼˜åŒ–ç‰ˆæœ¬"""
        visual_rois = []
        
        for step in range(max_steps):
            # æ„å»ºæŒ‡ä»¤
            if step == 0:
                instruction = self.vot_instruction_1.replace("<Q>", question)
            else:
                prev_roi = visual_rois[-1]
                instruction = (
                    f"<Img> Previous ROI: {prev_roi.bbox.to_string()} "
                    f"Confidence: {prev_roi.confidence:.2f}\n"
                    f"Question: {question}\n"
                    f"Please refine the ROI based on the previous result."
                )
            
            # è°ƒç”¨æ¨¡å‹
            response = self._call_model([image], instruction)
            
            # æå–ç»“æœ
            bbox, visualization = self._extract_coords_and_visualization(response)
            
            if bbox is None:
                bbox = self._heuristic_roi_extraction(image, question)
                visualization = self.spatial_visualizer.create_spatial_grid(bbox, f"æ­¥éª¤{step+1}")
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_step_confidence(response, bbox, step)
            
            # åˆ›å»ºVisualROI
            visual_roi = VisualROI(
                bbox=bbox,
                confidence=confidence,
                visualization=visualization,
                reasoning=response,
                step_id=step + 1
            )
            
            visual_rois.append(visual_roi)
            
            # å¦‚æœç½®ä¿¡åº¦è¶³å¤Ÿé«˜ï¼Œæå‰åœæ­¢
            if confidence > 0.8:
                break
            
            # NPUç¼“å­˜æ¸…ç†
            if self.device == "npu":
                self.npu_optimizer.clear_npu_cache()
        
        return visual_rois
    
    def _calculate_step_confidence(self, response: str, bbox: BoundingBox, step: int) -> float:
        """è®¡ç®—æ­¥éª¤ç½®ä¿¡åº¦"""
        confidence = 0.5
        
        if "COORDS:" in response and "VISUAL:" in response:
            confidence += 0.2
        
        roi_area = (bbox.x1 - bbox.x0) * (bbox.y1 - bbox.y0)
        if 0.1 <= roi_area <= 0.5:
            confidence += 0.1
        elif roi_area < 0.1:
            confidence -= 0.1
        
        confidence += step * 0.05
        
        return min(confidence, 1.0)
    
    def _dynamic_roi_adjustment(self, visual_rois: List[VisualROI]) -> BoundingBox:
        """åŠ¨æ€ROIè°ƒæ•´"""
        if not visual_rois:
            return BoundingBox(x0=0.25, x1=0.75, y0=0.25, y1=0.75)
        
        total_weight = 0
        weighted_x0 = 0
        weighted_x1 = 0
        weighted_y0 = 0
        weighted_y1 = 0
        
        for roi in visual_rois:
            weight = roi.confidence ** 2
            total_weight += weight
            
            weighted_x0 += roi.bbox.x0 * weight
            weighted_x1 += roi.bbox.x1 * weight
            weighted_y0 += roi.bbox.y0 * weight
            weighted_y1 += roi.bbox.y1 * weight
        
        if total_weight > 0:
            final_bbox = BoundingBox(
                x0=weighted_x0 / total_weight,
                x1=weighted_x1 / total_weight,
                y0=weighted_y0 / total_weight,
                y1=weighted_y1 / total_weight
            )
        else:
            final_bbox = visual_rois[-1].bbox
        
        return final_bbox
    
    def visual_interactive_reasoning(self, image: Image.Image, question: str) -> CoSVoTResponse:
        """å¯è§†åŒ–äº¤äº’å¼æ¨ç† - NPUä¼˜åŒ–ç‰ˆæœ¬"""
        reasoning_trace = []
        start_time = time.time()
        
        # æ­¥éª¤1: å¤šæ­¥éª¤ROIç»†åŒ–
        reasoning_trace.append("å¼€å§‹å¤šæ­¥éª¤ROIç»†åŒ–...")
        visual_rois = self._multi_step_roi_refinement(image, question)
        reasoning_trace.append(f"å®Œæˆ{len(visual_rois)}æ­¥ROIç»†åŒ–")
        
        # æ­¥éª¤2: åŠ¨æ€ROIè°ƒæ•´
        final_roi = self._dynamic_roi_adjustment(visual_rois)
        reasoning_trace.append(f"æœ€ç»ˆROI: {final_roi.to_string()}")
        
        # æ­¥éª¤3: è£å‰ªROIå›¾åƒ
        roi_image = self.image_cropper.crop_image(image, final_roi)
        reasoning_trace.append("ROIå›¾åƒè£å‰ªå®Œæˆ")
        
        # æ­¥éª¤4: åŸºäºROIçš„è¯¦ç»†åˆ†æ
        instruction_2 = self.vot_instruction_2.replace("<Q>", question)
        reasoning_trace.append("å¼€å§‹è¯¦ç»†åˆ†æ...")
        
        final_answer = self._call_model([image, roi_image], instruction_2)
        reasoning_trace.append("è¯¦ç»†åˆ†æå®Œæˆ")
        
        # æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆç©ºé—´å¯è§†åŒ–
        spatial_viz = self.spatial_visualizer.create_multi_roi_visualization(visual_rois)
        
        # è®¡ç®—æœ€ç»ˆç½®ä¿¡åº¦å’Œæ—¶é—´
        final_confidence = np.mean([roi.confidence for roi in visual_rois])
        total_time = time.time() - start_time
        
        reasoning_trace.append(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        return CoSVoTResponse(
            final_roi=final_roi,
            final_answer=final_answer,
            visual_trajectory=visual_rois,
            reasoning_trace=reasoning_trace,
            confidence=final_confidence,
            spatial_visualization=spatial_viz
        )


def cos_vot_npu_generate(
    model_id: str,
    image_path: str,
    question: str,
    device: str = "npu",
    dtype_str: str = "fp16",
    max_new_tokens: int = 512,
    max_roi_steps: int = 3,
    seed: int = None,
    save_visualization: bool = False,
    output_dir: str = ".",
) -> Dict[str, Any]:
    """CoS+VoT NPUç”Ÿæˆå‡½æ•°"""
    
    # NPUç¯å¢ƒæ£€æŸ¥
    if device == "npu" and not HAS_NPU:
        raise RuntimeError("NPUç¯å¢ƒæœªé…ç½®ï¼Œè¯·å®‰è£…torch_npu")
    
    if seed is not None:
        torch.manual_seed(seed)
    
    # NPUç¯å¢ƒè®¾ç½®
    if device == "npu":
        NPUOptimizer.setup_npu_environment()
    
    # æ•°æ®ç±»å‹é€‰æ‹©
    if dtype_str == "auto":
        if device == "npu":
            torch_dtype = torch.float16
        else:
            torch_dtype = torch.float32
    else:
        dtype_mapping = {"bf16": torch.bfloat16, "fp16": torch.float16, "fp32": torch.float32}
        torch_dtype = dtype_mapping.get(dtype_str, torch.float16)
    
    # åŠ è½½æ¨¡å‹
    try:
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        
        if device == "npu":
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id, 
                torch_dtype=torch_dtype,
                device_map=None
            )
            model = model.to("npu")
        else:
            model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                model_id, torch_dtype=torch_dtype, device_map="auto"
            )
        
        processor = AutoProcessor.from_pretrained(model_id)
    except:
        from transformers import AutoModelForCausalLM, AutoProcessor
        
        if device == "npu":
            model = AutoModelForCausalLM.from_pretrained(
                model_id, 
                torch_dtype=torch_dtype,
                device_map=None,
                trust_remote_code=True
            )
            model = model.to("npu")
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_id, torch_dtype=torch_dtype, device_map="auto", trust_remote_code=True
            )
        
        processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
    
    # åˆå§‹åŒ–NPUä¼˜åŒ–æ¨¡å‹
    cos_vot_model = CoSVoTNPUModel(model, processor, device)
    
    # åŠ è½½å›¾åƒ
    image = Image.open(image_path).convert("RGB")
    
    # æ‰§è¡Œå¯è§†åŒ–äº¤äº’å¼æ¨ç†
    response = cos_vot_model.visual_interactive_reasoning(image, question)
    
    # ä¿å­˜å¯è§†åŒ–ç»“æœ
    if save_visualization:
        viz_image = cos_vot_model.image_cropper.visualize_roi(image, response.final_roi)
        viz_path = f"{output_dir}/cos_vot_npu_visualization.png"
        viz_image.save(viz_path)
        
        viz_text_path = f"{output_dir}/spatial_visualization_npu.txt"
        with open(viz_text_path, 'w', encoding='utf-8') as f:
            f.write(response.spatial_visualization)
    
    # æ„å»ºè¿”å›ç»“æœ
    result = {
        "method": "CoS+VoT NPUä¼˜åŒ–ç‰ˆæœ¬",
        "device": device,
        "question": question,
        "image_path": image_path,
        "final_roi": response.final_roi.to_string(),
        "final_answer": response.final_answer,
        "confidence": response.confidence,
        "roi_steps": len(response.visual_trajectory),
        "visual_trajectory": [
            {
                "step": roi.step_id,
                "bbox": roi.bbox.to_string(),
                "confidence": roi.confidence,
                "visualization": roi.visualization,
                "reasoning": roi.reasoning
            }
            for roi in response.visual_trajectory
        ],
        "reasoning_trace": response.reasoning_trace,
        "spatial_visualization": response.spatial_visualization,
        "npu_optimizations": [
            "NPUå›¾ä¼˜åŒ–",
            "FP16ç²¾åº¦",
            "å†…å­˜ç®¡ç†",
            "ç¼“å­˜æ¸…ç†"
        ]
    }
    
    return result


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="CoS+VoT NPUä¼˜åŒ–ç‰ˆæœ¬")
    parser.add_argument("--image", required=True, help="è¾“å…¥å›¾åƒè·¯å¾„")
    parser.add_argument("--question", required=True, help="é—®é¢˜")
    parser.add_argument("--device", default="npu", choices=["npu", "cuda", "mps", "cpu"], help="è®¾å¤‡")
    parser.add_argument("--dtype", default="fp16", choices=["fp16", "fp32", "bf16"], help="æ•°æ®ç±»å‹")
    parser.add_argument("--max-roi-steps", type=int, default=3, help="æœ€å¤§ROIæ­¥éª¤æ•°")
    parser.add_argument("--save-viz", action="store_true", help="ä¿å­˜å¯è§†åŒ–")
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    try:
        result = cos_vot_npu_generate(
            model_id="Qwen/Qwen2.5-VL-3B-Instruct",
            image_path=args.image,
            question=args.question,
            device=args.device,
            dtype_str=args.dtype,
            max_roi_steps=args.max_roi_steps,
            save_visualization=args.save_viz
        )
        
        print("=" * 80)
        print("ğŸ”¬ CoS+VoT NPUä¼˜åŒ–ç‰ˆæœ¬ç»“æœ")
        print("=" * 80)
        print(f"è®¾å¤‡: {result['device']}")
        print(f"é—®é¢˜: {result['question']}")
        print(f"æœ€ç»ˆROI: {result['final_roi']}")
        print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
        print(f"ROIæ­¥éª¤æ•°: {result['roi_steps']}")
        print(f"æœ€ç»ˆç­”æ¡ˆ: {result['final_answer']}")
        
        print("\nğŸ“Š å¯è§†åŒ–è½¨è¿¹:")
        for step in result['visual_trajectory']:
            print(f"æ­¥éª¤{step['step']}: ROI={step['bbox']}, ç½®ä¿¡åº¦={step['confidence']:.3f}")
        
        print("\nğŸ¨ ç©ºé—´å¯è§†åŒ–:")
        print(result['spatial_visualization'])
        
        print("\nâœ¨ NPUä¼˜åŒ–ç‰¹æ€§:")
        for feature in result['npu_optimizations']:
            print(f"- {feature}")
        
    except Exception as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
