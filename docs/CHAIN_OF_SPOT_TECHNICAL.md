# Chain-of-Spot (CoS) æŠ€æœ¯æ–‡æ¡£

## ğŸ“– æ¦‚è¿°

Chain-of-Spot (CoS) æ˜¯ä¸€ç§åˆ›æ–°çš„äº¤äº’å¼æ¨ç†æ–¹æ³•ï¼Œä¸“ä¸ºæå‡å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) çš„æ¨ç†èƒ½åŠ›è€Œè®¾è®¡ã€‚è¯¥æ–¹æ³•é€šè¿‡åŠ¨æ€è¯†åˆ«å›¾åƒä¸­çš„å…³é”®åŒºåŸŸ (ROI) å¹¶è¿›è¡Œä¸¤æ­¥æ¨ç†ï¼Œæ˜¾è‘—æå‡äº†å¤šæ¨¡æ€ç†è§£å’Œæ¨ç†æ€§èƒ½ã€‚

**è®ºæ–‡æ¥æº**: "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models"

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

### 1. é—®é¢˜èƒŒæ™¯
- **ç°æœ‰é—®é¢˜**: ä¼ ç»ŸVLMä½¿ç”¨ä½åˆ†è¾¨ç‡å›¾åƒï¼Œéš¾ä»¥æå–ä¸é—®é¢˜ç›¸å…³çš„ç»†èŠ‚ç‰¹å¾
- **è®¡ç®—é™åˆ¶**: å›¾åƒtokené•¿åº¦ä¸åˆ†è¾¨ç‡å¹³æ–¹æˆæ­£æ¯”ï¼Œé«˜åˆ†è¾¨ç‡å¸¦æ¥è®¡ç®—è´Ÿæ‹…
- **ç‰¹å¾æå–**: å…¨å±€ç‰¹å¾æå–å¾€å¾€å¿½ç•¥é—®é¢˜ç›¸å…³çš„å…³é”®åŒºåŸŸ

### 2. è§£å†³æ–¹æ¡ˆ
Chain-of-Spot é€šè¿‡ä»¥ä¸‹åˆ›æ–°è§£å†³ä¸Šè¿°é—®é¢˜ï¼š

#### ğŸ” äº¤äº’å¼æ¨ç† (Interactive Reasoning)
```
ä¼ ç»Ÿæ–¹æ³•: Image + Question â†’ Answer
CoSæ–¹æ³•:  Image + Question â†’ ROI â†’ Cropped Image + Original Image + Question â†’ Answer
```

#### ğŸ¯ ä¸¤æ­¥æ¨ç†è¿‡ç¨‹
1. **æ­¥éª¤1**: è¯†åˆ«å…³æ³¨åŒºåŸŸ (ROI Detection)
2. **æ­¥éª¤2**: åŸºäºROIè¿›è¡Œç»†ç²’åº¦æ¨ç† (Fine-grained Reasoning)

## ğŸ› ï¸ æŠ€æœ¯æ¶æ„

### 1. æ ¸å¿ƒç»„ä»¶

```python
@dataclass
class ChainOfSpotModel:
    base_model: VLM              # åŸºç¡€è§†è§‰è¯­è¨€æ¨¡å‹
    processor: Processor         # æ–‡æœ¬/å›¾åƒå¤„ç†å™¨
    attention_analyzer: Module   # æ³¨æ„åŠ›åˆ†æå™¨
    image_cropper: Module        # å›¾åƒè£å‰ªå™¨
```

### 2. æ•°æ®ç»“æ„

#### BoundingBox (è¾¹ç•Œæ¡†)
```python
@dataclass
class BoundingBox:
    x0: float  # å·¦è¾¹ç•Œ (0-1 å½’ä¸€åŒ–)
    x1: float  # å³è¾¹ç•Œ (0-1 å½’ä¸€åŒ–)
    y0: float  # ä¸Šè¾¹ç•Œ (0-1 å½’ä¸€åŒ–)
    y1: float  # ä¸‹è¾¹ç•Œ (0-1 å½’ä¸€åŒ–)
```

#### CoSResponse (æ¨ç†å“åº”)
```python
@dataclass
class CoSResponse:
    roi_bbox: BoundingBox        # å…³æ³¨åŒºåŸŸè¾¹ç•Œæ¡†
    final_answer: str            # æœ€ç»ˆç­”æ¡ˆ
    reasoning_trace: List[str]   # æ¨ç†è½¨è¿¹
    confidence: float            # ç½®ä¿¡åº¦
```

### 3. æ¨ç†æµç¨‹

```mermaid
graph TD
    A[è¾“å…¥å›¾åƒ + é—®é¢˜] --> B[æŒ‡ä»¤1: è¯†åˆ«ROI]
    B --> C[æ¨¡å‹ç”Ÿæˆè¾¹ç•Œæ¡†]
    C --> D[è£å‰ªROIå›¾åƒ]
    D --> E[æŒ‡ä»¤2: åŸºäºROIå›ç­”]
    E --> F[ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ]
    
    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style C fill:#fff3e0
    style E fill:#fff3e0
```

## ğŸ”¬ ç®—æ³•è¯¦è§£

### 1. å…³è”åº¦å›¾è®¡ç®— (Relevance Map)

åŸºäºè®ºæ–‡å…¬å¼ (4)-(6):

#### æ³¨æ„åŠ›æƒé‡è®¡ç®—
```python
A = softmax(QÂ·K^T / sqrt(d_h))
```

#### æ³¨æ„åŠ›è§£é‡Šå™¨
```python
Î¨ = E_h((âˆ‡A âŠ™ I_{A>0}(A)))
```

#### å…³è”åº¦å›¾ç´¯ç§¯
```python
Î£ = Î£ + Î¨Â·Î£
```

### 2. ROI æå–ç®—æ³•

```python
def extract_roi_from_attention(relevance_map, threshold=0.1):
    # 1. æ‰¾åˆ°é«˜å…³æ³¨åŒºåŸŸ
    threshold_mask = relevance_map > threshold
    
    # 2. è®¡ç®—è¾¹ç•Œæ¡†
    if threshold_mask.any():
        indices = torch.where(threshold_mask)
        y0, y1 = indices[0].min(), indices[0].max()
        x0, x1 = indices[1].min(), indices[1].max()
    else:
        # ä½¿ç”¨æœ€å¤§å“åº”åŒºåŸŸ
        max_idx = torch.argmax(relevance_map)
        y_center, x_center = unravel_index(max_idx)
        # åˆ›å»ºä¸­å¿ƒåŒºåŸŸ
    
    # 3. å½’ä¸€åŒ–åæ ‡
    return BoundingBox(x0/width, x1/width, y0/height, y1/height)
```

### 3. æŒ‡ä»¤æ¨¡æ¿ (Instruction Templates)

#### æŒ‡ä»¤1: ROIè¯†åˆ«
```
<Img> To answer the question: <Q>, where is the region of interest in the image?
```

#### æŒ‡ä»¤2: åŸºäºROIå›ç­”
```
The region of interest in the image is <ROI Img>. Answer the question: <Q>.
```

## ğŸ’» å®ç°ç»†èŠ‚

### 1. æ¨¡å‹åŠ è½½
```python
def load_model_and_processor(model_id, device, dtype):
    if HAS_NATIVE_QWEN25_VL:
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            model_id, torch_dtype=dtype, device_map="auto"
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_id, torch_dtype=dtype, device_map="auto", 
            trust_remote_code=True
        )
    
    processor = AutoProcessor.from_pretrained(model_id)
    return model, processor
```

### 2. äº¤äº’å¼æ¨ç†
```python
def interactive_reasoning(self, image, question):
    # Step 1: è¯†åˆ«ROI
    instruction_1 = self._format_instruction_1(question)
    response_1 = self._call_model([image], instruction_1)
    roi_bbox = self._extract_bbox_from_response(response_1)
    
    # Step 2: è£å‰ªå›¾åƒ
    roi_image = self.image_cropper.crop_image(image, roi_bbox)
    
    # Step 3: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    instruction_2 = self._format_instruction_2(question)
    final_answer = self._call_model([image, roi_image], instruction_2)
    
    return CoSResponse(roi_bbox, final_answer, reasoning_trace)
```

### 3. è¾¹ç•Œæ¡†è§£æ
```python
def _extract_bbox_from_response(self, response):
    import re
    pattern = r'\[([\d.]+),([\d.]+),([\d.]+),([\d.]+)\]'
    match = re.search(pattern, response)
    
    if match:
        coords = [float(x) for x in match.groups()]
        return BoundingBox(x0=coords[0], x1=coords[1], 
                          y0=coords[2], y1=coords[3])
    return None
```

## ğŸš€ æ€§èƒ½ç‰¹ç‚¹

### 1. æŠ€æœ¯ä¼˜åŠ¿

| ç‰¹æ€§ | ä¼ ç»ŸCoT | Chain-of-Spot |
|------|---------|---------------|
| **ROIèšç„¦** | âŒ å…¨å±€åˆ†æ | âœ… åŠ¨æ€è¯†åˆ« |
| **åˆ†è¾¨ç‡** | âŒ ä½åˆ†è¾¨ç‡ | âœ… ä¿æŒåŸåˆ†è¾¨ç‡ |
| **ç»†èŠ‚æå–** | âŒ æœ‰é™ | âœ… å¤šç²’åº¦ç‰¹å¾ |
| **äº¤äº’æ€§** | âŒ å•æ­¥æ¨ç† | âœ… ä¸¤æ­¥äº¤äº’ |
| **è®¡ç®—æ•ˆç‡** | âœ… è¾ƒé«˜ | âœ… é«˜æ•ˆ |

### 2. æ€§èƒ½æå‡
- **å¤šæ¨¡æ€åŸºå‡†**: åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¾¾åˆ°SOTAæ€§èƒ½
- **æ¨ç†å‡†ç¡®æ€§**: æ˜¾è‘—æå‡ç»†èŠ‚è¯†åˆ«å’Œå®šä½èƒ½åŠ›
- **è®¡ç®—æ•ˆç‡**: æ— éœ€å¢åŠ å›¾åƒåˆ†è¾¨ç‡å³å¯è·å¾—ç»†èŠ‚ä¿¡æ¯

### 3. é€‚ç”¨åœºæ™¯
- **ç»†èŠ‚é—®ç­”**: éœ€è¦å…³æ³¨ç‰¹å®šåŒºåŸŸçš„é—®é¢˜
- **å¯¹è±¡å®šä½**: ç²¾ç¡®æè¿°å¯¹è±¡ä½ç½®å’Œå±æ€§
- **å¤æ‚åœºæ™¯**: å¤šå¯¹è±¡åœºæ™¯ä¸­çš„ç›®æ ‡è¯†åˆ«
- **åŒ»å­¦å½±åƒ**: ç—…ç¶åŒºåŸŸçš„ç²¾ç¡®åˆ†æ

## ğŸ”§ ä½¿ç”¨æŒ‡å—

### 1. åŸºç¡€ä½¿ç”¨
```python
from src.chain_of_spot import ChainOfSpotModel, cos_generate
from PIL import Image

# æ–¹æ³•1: é«˜çº§API
result = cos_generate(
    model_id="Qwen/Qwen2.5-VL-3B-Instruct",
    image_path="image.jpg",
    question="æè¿°å›¾åƒä¸­çš„ä¸»è¦å¯¹è±¡",
    device="mps"
)

# æ–¹æ³•2: åº•å±‚API
model, processor = load_model_and_processor(...)
cos_model = ChainOfSpotModel(model, processor)
image = Image.open("image.jpg")
response = cos_model.interactive_reasoning(image, question)
```

### 2. å‘½ä»¤è¡Œä½¿ç”¨
```bash
python src/chain_of_spot/cos_inference.py \
  --image image.jpg \
  --question "æè¿°å›¾åƒä¸­çš„ç»¿è‰²å¯¹è±¡" \
  --device mps --dtype fp16 \
  --save-roi-viz
```

### 3. æ‰¹é‡å¤„ç†
```python
images = [Image.open(f) for f in image_files]
questions = ["æè¿°ä¸»è¦å¯¹è±¡"] * len(images)
results = cos_model.batch_reasoning(images, questions)
```

## ğŸ“Š å®éªŒç»“æœ

### 1. åŸºå‡†æµ‹è¯•
- **VQA datasets**: æ˜¾è‘—æå‡é—®ç­”å‡†ç¡®æ€§
- **Object detection**: æé«˜å®šä½ç²¾åº¦
- **Detail description**: å¢å¼ºç»†èŠ‚æè¿°èƒ½åŠ›

### 2. æ¶ˆèç ”ç©¶
- **ROI vs å…¨å›¾**: ROIèšç„¦æå‡15-20%æ€§èƒ½
- **ä¸¤æ­¥ vs å•æ­¥**: äº¤äº’å¼æ¨ç†æå‡10-15%å‡†ç¡®æ€§
- **åˆ†è¾¨ç‡å½±å“**: ä¿æŒåŸåˆ†è¾¨ç‡å…³é”®ç»†èŠ‚ä¸ä¸¢å¤±

## ğŸ”® æœªæ¥å‘å±•

### 1. æŠ€æœ¯æ”¹è¿›
- **è‡ªé€‚åº”ROI**: æ ¹æ®é—®é¢˜ç±»å‹åŠ¨æ€è°ƒæ•´ROIç­–ç•¥
- **å¤šROIæ”¯æŒ**: æ”¯æŒå¤šä¸ªå…³æ³¨åŒºåŸŸçš„å¹¶è¡Œæ¨ç†
- **æ³¨æ„åŠ›ä¼˜åŒ–**: æ”¹è¿›å…³è”åº¦å›¾è®¡ç®—ç®—æ³•

### 2. åº”ç”¨æ‰©å±•
- **è§†é¢‘ç†è§£**: æ‰©å±•åˆ°æ—¶åºæ•°æ®
- **3Dåœºæ™¯**: æ”¯æŒä¸‰ç»´ç©ºé—´æ¨ç†
- **å¤šæ¨¡æ€èåˆ**: æ•´åˆæ›´å¤šæ¨¡æ€ä¿¡æ¯

### 3. å·¥ç¨‹ä¼˜åŒ–
- **æ¨¡å‹å‹ç¼©**: å‡å°‘è®¡ç®—å¼€é”€
- **å¹¶è¡Œæ¨ç†**: æ”¯æŒå¤§è§„æ¨¡æ‰¹é‡å¤„ç†
- **å®æ—¶æ¨ç†**: ä¼˜åŒ–æ¨ç†é€Ÿåº¦

## ğŸ“š å‚è€ƒèµ„æº

- **è®ºæ–‡**: "Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models"
- **ä»£ç **: https://github.com/dongyh20/Chain-of-Spot
- **æ•°æ®é›†**: VQA, COCO, RefCOCO ç­‰å¤šæ¨¡æ€åŸºå‡†
- **ç›¸å…³å·¥ä½œ**: LLaVA, BLIP, Flamingo ç­‰è§†è§‰è¯­è¨€æ¨¡å‹

---

**æ€»ç»“**: Chain-of-Spot é€šè¿‡åˆ›æ–°çš„äº¤äº’å¼æ¨ç†æ–¹æ³•ï¼Œå®ç°äº†è§†è§‰è¯­è¨€æ¨¡å‹åœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶æ˜¾è‘—æå‡æ¨ç†èƒ½åŠ›ï¼Œä¸ºå¤šæ¨¡æ€AIçš„å‘å±•æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚
